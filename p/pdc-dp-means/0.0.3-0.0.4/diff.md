# Comparing `tmp/pdc_dp_means-0.0.3-cp38-cp38-any.whl.zip` & `tmp/pdc_dp_means-0.0.4-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,11 +1,549 @@
-Zip file size: 131413 bytes, number of entries: 9
--rw-rw-r--  2.0 unx       59 b- defN 23-May-23 21:23 pdc_dp_means/__init__.py
--rwxrwxr-x  2.0 unx   328816 b- defN 23-May-27 17:26 pdc_dp_means/dp_means_cython.cpython-38-x86_64-linux-gnu.so
--rw-rw-r--  2.0 unx    48791 b- defN 23-May-26 21:32 pdc_dp_means/dpmeans.py
--rw-rw-r--  2.0 unx       21 b- defN 23-May-27 17:25 pdc_dp_means/release.py
--rw-rw-r--  2.0 unx     1518 b- defN 23-May-27 17:26 pdc_dp_means-0.0.3.dist-info/LICENSE
--rw-rw-r--  2.0 unx     5924 b- defN 23-May-27 17:26 pdc_dp_means-0.0.3.dist-info/METADATA
--rw-rw-r--  2.0 unx       94 b- defN 23-May-27 17:26 pdc_dp_means-0.0.3.dist-info/WHEEL
--rw-rw-r--  2.0 unx       13 b- defN 23-May-27 17:26 pdc_dp_means-0.0.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx      767 b- defN 23-May-27 17:26 pdc_dp_means-0.0.3.dist-info/RECORD
-9 files, 386003 bytes uncompressed, 130081 bytes compressed:  66.3%
+Zip file size: 2547613 bytes, number of entries: 547
+-rw-rw-rw-  2.0 fat       48 b- defN 23-May-28 20:46 pdc_dp_means/__init__.py
+-rw-rw-rw-  2.0 fat   205312 b- defN 23-May-28 20:55 pdc_dp_means/dp_means_cython.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    44995 b- defN 23-May-28 20:46 pdc_dp_means/dpmeans.py
+-rw-rw-rw-  2.0 fat       23 b- defN 23-May-28 20:46 pdc_dp_means/release.py
+-rw-rw-rw-  2.0 fat     4829 b- defN 23-May-28 20:48 sklearn/__init__.py
+-rw-rw-rw-  2.0 fat    10704 b- defN 23-May-28 20:48 sklearn/_config.py
+-rw-rw-rw-  2.0 fat      355 b- defN 23-May-28 20:48 sklearn/_distributor_init.py
+-rw-rw-rw-  2.0 fat     2720 b- defN 23-May-28 20:48 sklearn/_min_dependencies.py
+-rw-rw-rw-  2.0 fat    39707 b- defN 23-May-28 20:48 sklearn/base.py
+-rw-rw-rw-  2.0 fat    51204 b- defN 23-May-28 20:48 sklearn/calibration.py
+-rw-rw-rw-  2.0 fat     9264 b- defN 23-May-28 20:48 sklearn/conftest.py
+-rw-rw-rw-  2.0 fat    38231 b- defN 23-May-28 20:48 sklearn/discriminant_analysis.py
+-rw-rw-rw-  2.0 fat    24434 b- defN 23-May-28 20:48 sklearn/dummy.py
+-rw-rw-rw-  2.0 fat     4346 b- defN 23-May-28 20:48 sklearn/exceptions.py
+-rw-rw-rw-  2.0 fat    16030 b- defN 23-May-28 20:48 sklearn/isotonic.py
+-rw-rw-rw-  2.0 fat    39715 b- defN 23-May-28 20:48 sklearn/kernel_approximation.py
+-rw-rw-rw-  2.0 fat     9399 b- defN 23-May-28 20:48 sklearn/kernel_ridge.py
+-rw-rw-rw-  2.0 fat    37998 b- defN 23-May-28 20:48 sklearn/multiclass.py
+-rw-rw-rw-  2.0 fat    36147 b- defN 23-May-28 20:48 sklearn/multioutput.py
+-rw-rw-rw-  2.0 fat    57904 b- defN 23-May-28 20:48 sklearn/naive_bayes.py
+-rw-rw-rw-  2.0 fat    49469 b- defN 23-May-28 20:48 sklearn/pipeline.py
+-rw-rw-rw-  2.0 fat    28706 b- defN 23-May-28 20:48 sklearn/random_projection.py
+-rw-rw-rw-  2.0 fat     1752 b- defN 23-May-28 20:48 sklearn/__check_build/__init__.py
+-rw-rw-rw-  2.0 fat     4460 b- defN 23-May-28 20:48 sklearn/_build_utils/__init__.py
+-rw-rw-rw-  2.0 fat     4849 b- defN 23-May-28 20:48 sklearn/_build_utils/openmp_helpers.py
+-rw-rw-rw-  2.0 fat     2539 b- defN 23-May-28 20:48 sklearn/_build_utils/pre_build_helpers.py
+-rw-rw-rw-  2.0 fat      605 b- defN 23-May-28 20:48 sklearn/_loss/__init__.py
+-rw-rw-rw-  2.0 fat    12418 b- defN 23-May-28 20:48 sklearn/_loss/glm_distribution.py
+-rw-rw-rw-  2.0 fat     7932 b- defN 23-May-28 20:48 sklearn/_loss/link.py
+-rw-rw-rw-  2.0 fat    37300 b- defN 23-May-28 20:48 sklearn/_loss/loss.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/_loss/tests/__init__.py
+-rw-rw-rw-  2.0 fat     3959 b- defN 23-May-28 20:48 sklearn/_loss/tests/test_glm_distribution.py
+-rw-rw-rw-  2.0 fat     3928 b- defN 23-May-28 20:48 sklearn/_loss/tests/test_link.py
+-rw-rw-rw-  2.0 fat    43389 b- defN 23-May-28 20:48 sklearn/_loss/tests/test_loss.py
+-rw-rw-rw-  2.0 fat     1441 b- defN 23-May-28 20:48 sklearn/cluster/__init__.py
+-rw-rw-rw-  2.0 fat    20365 b- defN 23-May-28 20:48 sklearn/cluster/_affinity_propagation.py
+-rw-rw-rw-  2.0 fat    51232 b- defN 23-May-28 20:48 sklearn/cluster/_agglomerative.py
+-rw-rw-rw-  2.0 fat    22766 b- defN 23-May-28 20:48 sklearn/cluster/_bicluster.py
+-rw-rw-rw-  2.0 fat    26931 b- defN 23-May-28 20:48 sklearn/cluster/_birch.py
+-rw-rw-rw-  2.0 fat    19251 b- defN 23-May-28 20:48 sklearn/cluster/_bisect_k_means.py
+-rw-rw-rw-  2.0 fat    17783 b- defN 23-May-28 20:48 sklearn/cluster/_dbscan.py
+-rw-rw-rw-  2.0 fat     2466 b- defN 23-May-28 20:48 sklearn/cluster/_feature_agglomeration.py
+-rw-rw-rw-  2.0 fat    82593 b- defN 23-May-28 20:48 sklearn/cluster/_kmeans.py
+-rw-rw-rw-  2.0 fat    19460 b- defN 23-May-28 20:48 sklearn/cluster/_mean_shift.py
+-rw-rw-rw-  2.0 fat    41006 b- defN 23-May-28 20:48 sklearn/cluster/_optics.py
+-rw-rw-rw-  2.0 fat    31061 b- defN 23-May-28 20:48 sklearn/cluster/_spectral.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/cluster/tests/__init__.py
+-rw-rw-rw-  2.0 fat      919 b- defN 23-May-28 20:48 sklearn/cluster/tests/common.py
+-rw-rw-rw-  2.0 fat    11612 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_affinity_propagation.py
+-rw-rw-rw-  2.0 fat     9042 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_bicluster.py
+-rw-rw-rw-  2.0 fat     8821 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_birch.py
+-rw-rw-rw-  2.0 fat     4272 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_bisect_k_means.py
+-rw-rw-rw-  2.0 fat    14980 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_dbscan.py
+-rw-rw-rw-  2.0 fat     2049 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_feature_agglomeration.py
+-rw-rw-rw-  2.0 fat    34013 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_hierarchical.py
+-rw-rw-rw-  2.0 fat    46639 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_k_means.py
+-rw-rw-rw-  2.0 fat     7388 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_mean_shift.py
+-rw-rw-rw-  2.0 fat    23103 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_optics.py
+-rw-rw-rw-  2.0 fat    11834 b- defN 23-May-28 20:48 sklearn/cluster/tests/test_spectral.py
+-rw-rw-rw-  2.0 fat      519 b- defN 23-May-28 20:48 sklearn/compose/__init__.py
+-rw-rw-rw-  2.0 fat    45598 b- defN 23-May-28 20:48 sklearn/compose/_column_transformer.py
+-rw-rw-rw-  2.0 fat    11848 b- defN 23-May-28 20:48 sklearn/compose/_target.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/compose/tests/__init__.py
+-rw-rw-rw-  2.0 fat    76219 b- defN 23-May-28 20:48 sklearn/compose/tests/test_column_transformer.py
+-rw-rw-rw-  2.0 fat    13673 b- defN 23-May-28 20:48 sklearn/compose/tests/test_target.py
+-rw-rw-rw-  2.0 fat     1162 b- defN 23-May-28 20:48 sklearn/covariance/__init__.py
+-rw-rw-rw-  2.0 fat     9291 b- defN 23-May-28 20:48 sklearn/covariance/_elliptic_envelope.py
+-rw-rw-rw-  2.0 fat    11998 b- defN 23-May-28 20:48 sklearn/covariance/_empirical_covariance.py
+-rw-rw-rw-  2.0 fat    36325 b- defN 23-May-28 20:48 sklearn/covariance/_graph_lasso.py
+-rw-rw-rw-  2.0 fat    34665 b- defN 23-May-28 20:48 sklearn/covariance/_robust_covariance.py
+-rw-rw-rw-  2.0 fat    26966 b- defN 23-May-28 20:48 sklearn/covariance/_shrunk_covariance.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/covariance/tests/__init__.py
+-rw-rw-rw-  2.0 fat    13135 b- defN 23-May-28 20:48 sklearn/covariance/tests/test_covariance.py
+-rw-rw-rw-  2.0 fat     1688 b- defN 23-May-28 20:48 sklearn/covariance/tests/test_elliptic_envelope.py
+-rw-rw-rw-  2.0 fat     8900 b- defN 23-May-28 20:48 sklearn/covariance/tests/test_graphical_lasso.py
+-rw-rw-rw-  2.0 fat     6359 b- defN 23-May-28 20:48 sklearn/covariance/tests/test_robust_covariance.py
+-rw-rw-rw-  2.0 fat      124 b- defN 23-May-28 20:48 sklearn/cross_decomposition/__init__.py
+-rw-rw-rw-  2.0 fat    38107 b- defN 23-May-28 20:48 sklearn/cross_decomposition/_pls.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/cross_decomposition/tests/__init__.py
+-rw-rw-rw-  2.0 fat    22647 b- defN 23-May-28 20:48 sklearn/cross_decomposition/tests/test_pls.py
+-rw-rw-rw-  2.0 fat     6084 b- defN 23-May-28 20:48 sklearn/datasets/__init__.py
+-rw-rw-rw-  2.0 fat    18943 b- defN 23-May-28 20:48 sklearn/datasets/_arff_parser.py
+-rw-rw-rw-  2.0 fat    45051 b- defN 23-May-28 20:48 sklearn/datasets/_base.py
+-rw-rw-rw-  2.0 fat     6319 b- defN 23-May-28 20:48 sklearn/datasets/_california_housing.py
+-rw-rw-rw-  2.0 fat     7150 b- defN 23-May-28 20:48 sklearn/datasets/_covtype.py
+-rw-rw-rw-  2.0 fat    13119 b- defN 23-May-28 20:48 sklearn/datasets/_kddcup99.py
+-rw-rw-rw-  2.0 fat    20015 b- defN 23-May-28 20:48 sklearn/datasets/_lfw.py
+-rw-rw-rw-  2.0 fat     5176 b- defN 23-May-28 20:48 sklearn/datasets/_olivetti_faces.py
+-rw-rw-rw-  2.0 fat    41123 b- defN 23-May-28 20:48 sklearn/datasets/_openml.py
+-rw-rw-rw-  2.0 fat    11023 b- defN 23-May-28 20:48 sklearn/datasets/_rcv1.py
+-rw-rw-rw-  2.0 fat    64636 b- defN 23-May-28 20:48 sklearn/datasets/_samples_generator.py
+-rw-rw-rw-  2.0 fat     8715 b- defN 23-May-28 20:48 sklearn/datasets/_species_distributions.py
+-rw-rw-rw-  2.0 fat    19310 b- defN 23-May-28 20:48 sklearn/datasets/_svmlight_format_io.py
+-rw-rw-rw-  2.0 fat    18896 b- defN 23-May-28 20:48 sklearn/datasets/_twenty_newsgroups.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/data/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/descr/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/images/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/__init__.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-May-28 20:48 sklearn/datasets/tests/conftest.py
+-rw-rw-rw-  2.0 fat     5481 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_20news.py
+-rw-rw-rw-  2.0 fat     8480 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_arff_parser.py
+-rw-rw-rw-  2.0 fat    12258 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_base.py
+-rw-rw-rw-  2.0 fat     1403 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_california_housing.py
+-rw-rw-rw-  2.0 fat     4514 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_common.py
+-rw-rw-rw-  2.0 fat     1750 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_covtype.py
+-rw-rw-rw-  2.0 fat     2766 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_kddcup99.py
+-rw-rw-rw-  2.0 fat     8499 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_lfw.py
+-rw-rw-rw-  2.0 fat      947 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_olivetti_faces.py
+-rw-rw-rw-  2.0 fat    57039 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_openml.py
+-rw-rw-rw-  2.0 fat     2393 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_rcv1.py
+-rw-rw-rw-  2.0 fat    23958 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_samples_generator.py
+-rw-rw-rw-  2.0 fat    19915 b- defN 23-May-28 20:48 sklearn/datasets/tests/test_svmlight_format.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_1/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_1119/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_1590/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_2/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_292/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_3/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_40589/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_40675/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_40945/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_40966/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_42074/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_42585/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_561/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_61/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/datasets/tests/data/openml/id_62/__init__.py
+-rw-rw-rw-  2.0 fat     1350 b- defN 23-May-28 20:48 sklearn/decomposition/__init__.py
+-rw-rw-rw-  2.0 fat     5892 b- defN 23-May-28 20:48 sklearn/decomposition/_base.py
+-rw-rw-rw-  2.0 fat    85985 b- defN 23-May-28 20:48 sklearn/decomposition/_dict_learning.py
+-rw-rw-rw-  2.0 fat    15699 b- defN 23-May-28 20:48 sklearn/decomposition/_factor_analysis.py
+-rw-rw-rw-  2.0 fat    27500 b- defN 23-May-28 20:48 sklearn/decomposition/_fastica.py
+-rw-rw-rw-  2.0 fat    16151 b- defN 23-May-28 20:48 sklearn/decomposition/_incremental_pca.py
+-rw-rw-rw-  2.0 fat    21948 b- defN 23-May-28 20:48 sklearn/decomposition/_kernel_pca.py
+-rw-rw-rw-  2.0 fat    33900 b- defN 23-May-28 20:48 sklearn/decomposition/_lda.py
+-rw-rw-rw-  2.0 fat    79942 b- defN 23-May-28 20:48 sklearn/decomposition/_nmf.py
+-rw-rw-rw-  2.0 fat    26486 b- defN 23-May-28 20:48 sklearn/decomposition/_pca.py
+-rw-rw-rw-  2.0 fat    18745 b- defN 23-May-28 20:48 sklearn/decomposition/_sparse_pca.py
+-rw-rw-rw-  2.0 fat    11797 b- defN 23-May-28 20:48 sklearn/decomposition/_truncated_svd.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/decomposition/tests/__init__.py
+-rw-rw-rw-  2.0 fat    34982 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_dict_learning.py
+-rw-rw-rw-  2.0 fat     4337 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_factor_analysis.py
+-rw-rw-rw-  2.0 fat    17978 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_fastica.py
+-rw-rw-rw-  2.0 fat    15763 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_incremental_pca.py
+-rw-rw-rw-  2.0 fat    20011 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_kernel_pca.py
+-rw-rw-rw-  2.0 fat    29365 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_nmf.py
+-rw-rw-rw-  2.0 fat    14995 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_online_lda.py
+-rw-rw-rw-  2.0 fat    24889 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_pca.py
+-rw-rw-rw-  2.0 fat    13827 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_sparse_pca.py
+-rw-rw-rw-  2.0 fat     7382 b- defN 23-May-28 20:48 sklearn/decomposition/tests/test_truncated_svd.py
+-rw-rw-rw-  2.0 fat     1549 b- defN 23-May-28 20:48 sklearn/ensemble/__init__.py
+-rw-rw-rw-  2.0 fat    45118 b- defN 23-May-28 20:48 sklearn/ensemble/_bagging.py
+-rw-rw-rw-  2.0 fat    11992 b- defN 23-May-28 20:48 sklearn/ensemble/_base.py
+-rw-rw-rw-  2.0 fat   110218 b- defN 23-May-28 20:48 sklearn/ensemble/_forest.py
+-rw-rw-rw-  2.0 fat    73908 b- defN 23-May-28 20:48 sklearn/ensemble/_gb.py
+-rw-rw-rw-  2.0 fat    32425 b- defN 23-May-28 20:48 sklearn/ensemble/_gb_losses.py
+-rw-rw-rw-  2.0 fat    20217 b- defN 23-May-28 20:48 sklearn/ensemble/_iforest.py
+-rw-rw-rw-  2.0 fat    39631 b- defN 23-May-28 20:48 sklearn/ensemble/_stacking.py
+-rw-rw-rw-  2.0 fat    22904 b- defN 23-May-28 20:48 sklearn/ensemble/_voting.py
+-rw-rw-rw-  2.0 fat    46263 b- defN 23-May-28 20:48 sklearn/ensemble/_weight_boosting.py
+-rw-rw-rw-  2.0 fat      171 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/__init__.py
+-rw-rw-rw-  2.0 fat    13614 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/binning.py
+-rw-rw-rw-  2.0 fat    84574 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py
+-rw-rw-rw-  2.0 fat    31555 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/grower.py
+-rw-rw-rw-  2.0 fat     4174 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/predictor.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/__init__.py
+-rw-rw-rw-  2.0 fat    15775 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py
+-rw-rw-rw-  2.0 fat     2164 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py
+-rw-rw-rw-  2.0 fat     9341 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py
+-rw-rw-rw-  2.0 fat    50617 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
+-rw-rw-rw-  2.0 fat    24041 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py
+-rw-rw-rw-  2.0 fat     9121 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py
+-rw-rw-rw-  2.0 fat    16386 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py
+-rw-rw-rw-  2.0 fat     6532 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py
+-rw-rw-rw-  2.0 fat    35705 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py
+-rw-rw-rw-  2.0 fat     8209 b- defN 23-May-28 20:48 sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/ensemble/tests/__init__.py
+-rw-rw-rw-  2.0 fat    32396 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_bagging.py
+-rw-rw-rw-  2.0 fat     4847 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_base.py
+-rw-rw-rw-  2.0 fat     9476 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_common.py
+-rw-rw-rw-  2.0 fat    59472 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_forest.py
+-rw-rw-rw-  2.0 fat    47238 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_gradient_boosting.py
+-rw-rw-rw-  2.0 fat    12967 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+-rw-rw-rw-  2.0 fat    11999 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_iforest.py
+-rw-rw-rw-  2.0 fat    29305 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_stacking.py
+-rw-rw-rw-  2.0 fat    24173 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_voting.py
+-rw-rw-rw-  2.0 fat    23640 b- defN 23-May-28 20:48 sklearn/ensemble/tests/test_weight_boosting.py
+-rw-rw-rw-  2.0 fat      259 b- defN 23-May-28 20:48 sklearn/experimental/__init__.py
+-rw-rw-rw-  2.0 fat     1244 b- defN 23-May-28 20:48 sklearn/experimental/enable_halving_search_cv.py
+-rw-rw-rw-  2.0 fat      768 b- defN 23-May-28 20:48 sklearn/experimental/enable_hist_gradient_boosting.py
+-rw-rw-rw-  2.0 fat      708 b- defN 23-May-28 20:48 sklearn/experimental/enable_iterative_imputer.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/experimental/tests/__init__.py
+-rw-rw-rw-  2.0 fat      439 b- defN 23-May-28 20:48 sklearn/experimental/tests/test_enable_hist_gradient_boosting.py
+-rw-rw-rw-  2.0 fat     1435 b- defN 23-May-28 20:48 sklearn/experimental/tests/test_enable_iterative_imputer.py
+-rw-rw-rw-  2.0 fat     1640 b- defN 23-May-28 20:48 sklearn/experimental/tests/test_enable_successive_halving.py
+-rw-rw-rw-  2.0 fat       47 b- defN 23-May-28 20:48 sklearn/externals/__init__.py
+-rw-rw-rw-  2.0 fat    39448 b- defN 23-May-28 20:48 sklearn/externals/_arff.py
+-rw-rw-rw-  2.0 fat    38841 b- defN 23-May-28 20:48 sklearn/externals/_lobpcg.py
+-rw-rw-rw-  2.0 fat      309 b- defN 23-May-28 20:48 sklearn/externals/conftest.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/externals/_packaging/__init__.py
+-rw-rw-rw-  2.0 fat     3012 b- defN 23-May-28 20:48 sklearn/externals/_packaging/_structures.py
+-rw-rw-rw-  2.0 fat    16669 b- defN 23-May-28 20:48 sklearn/externals/_packaging/version.py
+-rw-rw-rw-  2.0 fat      458 b- defN 23-May-28 20:48 sklearn/feature_extraction/__init__.py
+-rw-rw-rw-  2.0 fat    15694 b- defN 23-May-28 20:48 sklearn/feature_extraction/_dict_vectorizer.py
+-rw-rw-rw-  2.0 fat     7466 b- defN 23-May-28 20:48 sklearn/feature_extraction/_hash.py
+-rw-rw-rw-  2.0 fat     5970 b- defN 23-May-28 20:48 sklearn/feature_extraction/_stop_words.py
+-rw-rw-rw-  2.0 fat    20329 b- defN 23-May-28 20:48 sklearn/feature_extraction/image.py
+-rw-rw-rw-  2.0 fat    79386 b- defN 23-May-28 20:48 sklearn/feature_extraction/text.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/feature_extraction/tests/__init__.py
+-rw-rw-rw-  2.0 fat     7852 b- defN 23-May-28 20:48 sklearn/feature_extraction/tests/test_dict_vectorizer.py
+-rw-rw-rw-  2.0 fat     5198 b- defN 23-May-28 20:48 sklearn/feature_extraction/tests/test_feature_hasher.py
+-rw-rw-rw-  2.0 fat    12130 b- defN 23-May-28 20:48 sklearn/feature_extraction/tests/test_image.py
+-rw-rw-rw-  2.0 fat    54586 b- defN 23-May-28 20:48 sklearn/feature_extraction/tests/test_text.py
+-rw-rw-rw-  2.0 fat     1479 b- defN 23-May-28 20:48 sklearn/feature_selection/__init__.py
+-rw-rw-rw-  2.0 fat     8705 b- defN 23-May-28 20:48 sklearn/feature_selection/_base.py
+-rw-rw-rw-  2.0 fat    16292 b- defN 23-May-28 20:48 sklearn/feature_selection/_from_model.py
+-rw-rw-rw-  2.0 fat    17084 b- defN 23-May-28 20:48 sklearn/feature_selection/_mutual_info.py
+-rw-rw-rw-  2.0 fat    27676 b- defN 23-May-28 20:48 sklearn/feature_selection/_rfe.py
+-rw-rw-rw-  2.0 fat    12901 b- defN 23-May-28 20:48 sklearn/feature_selection/_sequential.py
+-rw-rw-rw-  2.0 fat    37640 b- defN 23-May-28 20:48 sklearn/feature_selection/_univariate_selection.py
+-rw-rw-rw-  2.0 fat     4565 b- defN 23-May-28 20:48 sklearn/feature_selection/_variance_threshold.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/__init__.py
+-rw-rw-rw-  2.0 fat     3710 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_base.py
+-rw-rw-rw-  2.0 fat     3027 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_chi2.py
+-rw-rw-rw-  2.0 fat    31308 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_feature_select.py
+-rw-rw-rw-  2.0 fat    23039 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_from_model.py
+-rw-rw-rw-  2.0 fat     8771 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_mutual_info.py
+-rw-rw-rw-  2.0 fat    19920 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_rfe.py
+-rw-rw-rw-  2.0 fat    10908 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_sequential.py
+-rw-rw-rw-  2.0 fat     2130 b- defN 23-May-28 20:48 sklearn/feature_selection/tests/test_variance_threshold.py
+-rw-rw-rw-  2.0 fat      521 b- defN 23-May-28 20:48 sklearn/gaussian_process/__init__.py
+-rw-rw-rw-  2.0 fat    37373 b- defN 23-May-28 20:48 sklearn/gaussian_process/_gpc.py
+-rw-rw-rw-  2.0 fat    27222 b- defN 23-May-28 20:48 sklearn/gaussian_process/_gpr.py
+-rw-rw-rw-  2.0 fat    86923 b- defN 23-May-28 20:48 sklearn/gaussian_process/kernels.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/gaussian_process/tests/__init__.py
+-rw-rw-rw-  2.0 fat     1687 b- defN 23-May-28 20:48 sklearn/gaussian_process/tests/_mini_sequence_kernel.py
+-rw-rw-rw-  2.0 fat    10144 b- defN 23-May-28 20:48 sklearn/gaussian_process/tests/test_gpc.py
+-rw-rw-rw-  2.0 fat    28760 b- defN 23-May-28 20:48 sklearn/gaussian_process/tests/test_gpr.py
+-rw-rw-rw-  2.0 fat    14002 b- defN 23-May-28 20:48 sklearn/gaussian_process/tests/test_kernels.py
+-rw-rw-rw-  2.0 fat      967 b- defN 23-May-28 20:48 sklearn/impute/__init__.py
+-rw-rw-rw-  2.0 fat    40571 b- defN 23-May-28 20:48 sklearn/impute/_base.py
+-rw-rw-rw-  2.0 fat    35632 b- defN 23-May-28 20:48 sklearn/impute/_iterative.py
+-rw-rw-rw-  2.0 fat    14478 b- defN 23-May-28 20:48 sklearn/impute/_knn.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/impute/tests/__init__.py
+-rw-rw-rw-  2.0 fat     3478 b- defN 23-May-28 20:48 sklearn/impute/tests/test_base.py
+-rw-rw-rw-  2.0 fat     6548 b- defN 23-May-28 20:48 sklearn/impute/tests/test_common.py
+-rw-rw-rw-  2.0 fat    59085 b- defN 23-May-28 20:48 sklearn/impute/tests/test_impute.py
+-rw-rw-rw-  2.0 fat    17230 b- defN 23-May-28 20:48 sklearn/impute/tests/test_knn.py
+-rw-rw-rw-  2.0 fat      470 b- defN 23-May-28 20:48 sklearn/inspection/__init__.py
+-rw-rw-rw-  2.0 fat    24240 b- defN 23-May-28 20:48 sklearn/inspection/_partial_dependence.py
+-rw-rw-rw-  2.0 fat     2201 b- defN 23-May-28 20:48 sklearn/inspection/_pd_utils.py
+-rw-rw-rw-  2.0 fat    10851 b- defN 23-May-28 20:48 sklearn/inspection/_permutation_importance.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/inspection/_plot/__init__.py
+-rw-rw-rw-  2.0 fat    13804 b- defN 23-May-28 20:48 sklearn/inspection/_plot/decision_boundary.py
+-rw-rw-rw-  2.0 fat    62268 b- defN 23-May-28 20:48 sklearn/inspection/_plot/partial_dependence.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/inspection/_plot/tests/__init__.py
+-rw-rw-rw-  2.0 fat    12109 b- defN 23-May-28 20:48 sklearn/inspection/_plot/tests/test_boundary_decision_display.py
+-rw-rw-rw-  2.0 fat    37733 b- defN 23-May-28 20:48 sklearn/inspection/_plot/tests/test_plot_partial_dependence.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/inspection/tests/__init__.py
+-rw-rw-rw-  2.0 fat    30812 b- defN 23-May-28 20:48 sklearn/inspection/tests/test_partial_dependence.py
+-rw-rw-rw-  2.0 fat     1689 b- defN 23-May-28 20:48 sklearn/inspection/tests/test_pd_utils.py
+-rw-rw-rw-  2.0 fat    20596 b- defN 23-May-28 20:48 sklearn/inspection/tests/test_permutation_importance.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 23-May-28 20:48 sklearn/linear_model/__init__.py
+-rw-rw-rw-  2.0 fat    30373 b- defN 23-May-28 20:48 sklearn/linear_model/_base.py
+-rw-rw-rw-  2.0 fat    27227 b- defN 23-May-28 20:48 sklearn/linear_model/_bayes.py
+-rw-rw-rw-  2.0 fat   107164 b- defN 23-May-28 20:48 sklearn/linear_model/_coordinate_descent.py
+-rw-rw-rw-  2.0 fat    12662 b- defN 23-May-28 20:48 sklearn/linear_model/_huber.py
+-rw-rw-rw-  2.0 fat    86262 b- defN 23-May-28 20:48 sklearn/linear_model/_least_angle.py
+-rw-rw-rw-  2.0 fat    26704 b- defN 23-May-28 20:48 sklearn/linear_model/_linear_loss.py
+-rw-rw-rw-  2.0 fat    83684 b- defN 23-May-28 20:48 sklearn/linear_model/_logistic.py
+-rw-rw-rw-  2.0 fat    38975 b- defN 23-May-28 20:48 sklearn/linear_model/_omp.py
+-rw-rw-rw-  2.0 fat    19751 b- defN 23-May-28 20:48 sklearn/linear_model/_passive_aggressive.py
+-rw-rw-rw-  2.0 fat     7584 b- defN 23-May-28 20:48 sklearn/linear_model/_perceptron.py
+-rw-rw-rw-  2.0 fat    11487 b- defN 23-May-28 20:48 sklearn/linear_model/_quantile.py
+-rw-rw-rw-  2.0 fat    23104 b- defN 23-May-28 20:48 sklearn/linear_model/_ransac.py
+-rw-rw-rw-  2.0 fat    92964 b- defN 23-May-28 20:48 sklearn/linear_model/_ridge.py
+-rw-rw-rw-  2.0 fat    12692 b- defN 23-May-28 20:48 sklearn/linear_model/_sag.py
+-rw-rw-rw-  2.0 fat    90259 b- defN 23-May-28 20:48 sklearn/linear_model/_stochastic_gradient.py
+-rw-rw-rw-  2.0 fat    16234 b- defN 23-May-28 20:48 sklearn/linear_model/_theil_sen.py
+-rw-rw-rw-  2.0 fat      278 b- defN 23-May-28 20:48 sklearn/linear_model/_glm/__init__.py
+-rw-rw-rw-  2.0 fat    19599 b- defN 23-May-28 20:48 sklearn/linear_model/_glm/_newton_solver.py
+-rw-rw-rw-  2.0 fat    33843 b- defN 23-May-28 20:48 sklearn/linear_model/_glm/glm.py
+-rw-rw-rw-  2.0 fat       25 b- defN 23-May-28 20:48 sklearn/linear_model/_glm/tests/__init__.py
+-rw-rw-rw-  2.0 fat    42621 b- defN 23-May-28 20:48 sklearn/linear_model/_glm/tests/test_glm.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/linear_model/tests/__init__.py
+-rw-rw-rw-  2.0 fat    25584 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_base.py
+-rw-rw-rw-  2.0 fat    10631 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_bayes.py
+-rw-rw-rw-  2.0 fat     4826 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_common.py
+-rw-rw-rw-  2.0 fat    57055 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_coordinate_descent.py
+-rw-rw-rw-  2.0 fat     7674 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_huber.py
+-rw-rw-rw-  2.0 fat    31864 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_least_angle.py
+-rw-rw-rw-  2.0 fat    13074 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_linear_loss.py
+-rw-rw-rw-  2.0 fat    71133 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_logistic.py
+-rw-rw-rw-  2.0 fat    10449 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_omp.py
+-rw-rw-rw-  2.0 fat     9177 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_passive_aggressive.py
+-rw-rw-rw-  2.0 fat     2701 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_perceptron.py
+-rw-rw-rw-  2.0 fat    11619 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_quantile.py
+-rw-rw-rw-  2.0 fat    18572 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_ransac.py
+-rw-rw-rw-  2.0 fat    70113 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_ridge.py
+-rw-rw-rw-  2.0 fat    31542 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_sag.py
+-rw-rw-rw-  2.0 fat    71704 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_sgd.py
+-rw-rw-rw-  2.0 fat    12363 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_sparse_coordinate_descent.py
+-rw-rw-rw-  2.0 fat    10199 b- defN 23-May-28 20:48 sklearn/linear_model/tests/test_theil_sen.py
+-rw-rw-rw-  2.0 fat      554 b- defN 23-May-28 20:48 sklearn/manifold/__init__.py
+-rw-rw-rw-  2.0 fat    15786 b- defN 23-May-28 20:48 sklearn/manifold/_isomap.py
+-rw-rw-rw-  2.0 fat    29900 b- defN 23-May-28 20:48 sklearn/manifold/_locally_linear.py
+-rw-rw-rw-  2.0 fat    23291 b- defN 23-May-28 20:48 sklearn/manifold/_mds.py
+-rw-rw-rw-  2.0 fat    28065 b- defN 23-May-28 20:48 sklearn/manifold/_spectral_embedding.py
+-rw-rw-rw-  2.0 fat    44511 b- defN 23-May-28 20:48 sklearn/manifold/_t_sne.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/manifold/tests/__init__.py
+-rw-rw-rw-  2.0 fat    12311 b- defN 23-May-28 20:48 sklearn/manifold/tests/test_isomap.py
+-rw-rw-rw-  2.0 fat     5919 b- defN 23-May-28 20:48 sklearn/manifold/tests/test_locally_linear.py
+-rw-rw-rw-  2.0 fat     3821 b- defN 23-May-28 20:48 sklearn/manifold/tests/test_mds.py
+-rw-rw-rw-  2.0 fat    17924 b- defN 23-May-28 20:48 sklearn/manifold/tests/test_spectral_embedding.py
+-rw-rw-rw-  2.0 fat    40791 b- defN 23-May-28 20:48 sklearn/manifold/tests/test_t_sne.py
+-rw-rw-rw-  2.0 fat     5917 b- defN 23-May-28 20:48 sklearn/metrics/__init__.py
+-rw-rw-rw-  2.0 fat     9243 b- defN 23-May-28 20:48 sklearn/metrics/_base.py
+-rw-rw-rw-  2.0 fat   111534 b- defN 23-May-28 20:48 sklearn/metrics/_classification.py
+-rw-rw-rw-  2.0 fat    71783 b- defN 23-May-28 20:48 sklearn/metrics/_ranking.py
+-rw-rw-rw-  2.0 fat    53287 b- defN 23-May-28 20:48 sklearn/metrics/_regression.py
+-rw-rw-rw-  2.0 fat    31425 b- defN 23-May-28 20:48 sklearn/metrics/_scorer.py
+-rw-rw-rw-  2.0 fat    78310 b- defN 23-May-28 20:48 sklearn/metrics/pairwise.py
+-rw-rw-rw-  2.0 fat     4568 b- defN 23-May-28 20:48 sklearn/metrics/_pairwise_distances_reduction/__init__.py
+-rw-rw-rw-  2.0 fat    17083 b- defN 23-May-28 20:48 sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/metrics/_plot/__init__.py
+-rw-rw-rw-  2.0 fat     4175 b- defN 23-May-28 20:48 sklearn/metrics/_plot/base.py
+-rw-rw-rw-  2.0 fat    16837 b- defN 23-May-28 20:48 sklearn/metrics/_plot/confusion_matrix.py
+-rw-rw-rw-  2.0 fat    11311 b- defN 23-May-28 20:48 sklearn/metrics/_plot/det_curve.py
+-rw-rw-rw-  2.0 fat    13880 b- defN 23-May-28 20:48 sklearn/metrics/_plot/precision_recall_curve.py
+-rw-rw-rw-  2.0 fat    14828 b- defN 23-May-28 20:48 sklearn/metrics/_plot/regression.py
+-rw-rw-rw-  2.0 fat    11896 b- defN 23-May-28 20:48 sklearn/metrics/_plot/roc_curve.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/__init__.py
+-rw-rw-rw-  2.0 fat     2607 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_base.py
+-rw-rw-rw-  2.0 fat     4754 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_common_curve_display.py
+-rw-rw-rw-  2.0 fat    15099 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_confusion_matrix_display.py
+-rw-rw-rw-  2.0 fat     3562 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_det_curve_display.py
+-rw-rw-rw-  2.0 fat    11405 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_precision_recall_display.py
+-rw-rw-rw-  2.0 fat     5951 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_predict_error_display.py
+-rw-rw-rw-  2.0 fat     8015 b- defN 23-May-28 20:48 sklearn/metrics/_plot/tests/test_roc_curve_display.py
+-rw-rw-rw-  2.0 fat     1752 b- defN 23-May-28 20:48 sklearn/metrics/cluster/__init__.py
+-rw-rw-rw-  2.0 fat     2920 b- defN 23-May-28 20:48 sklearn/metrics/cluster/_bicluster.py
+-rw-rw-rw-  2.0 fat    42388 b- defN 23-May-28 20:48 sklearn/metrics/cluster/_supervised.py
+-rw-rw-rw-  2.0 fat    14218 b- defN 23-May-28 20:48 sklearn/metrics/cluster/_unsupervised.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/metrics/cluster/tests/__init__.py
+-rw-rw-rw-  2.0 fat     1777 b- defN 23-May-28 20:48 sklearn/metrics/cluster/tests/test_bicluster.py
+-rw-rw-rw-  2.0 fat     8308 b- defN 23-May-28 20:48 sklearn/metrics/cluster/tests/test_common.py
+-rw-rw-rw-  2.0 fat    18799 b- defN 23-May-28 20:48 sklearn/metrics/cluster/tests/test_supervised.py
+-rw-rw-rw-  2.0 fat    10891 b- defN 23-May-28 20:48 sklearn/metrics/cluster/tests/test_unsupervised.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/metrics/tests/__init__.py
+-rw-rw-rw-  2.0 fat    98486 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_classification.py
+-rw-rw-rw-  2.0 fat    59840 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_common.py
+-rw-rw-rw-  2.0 fat    16920 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_dist_metrics.py
+-rw-rw-rw-  2.0 fat    56689 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_pairwise.py
+-rw-rw-rw-  2.0 fat    42564 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_pairwise_distances_reduction.py
+-rw-rw-rw-  2.0 fat    80492 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_ranking.py
+-rw-rw-rw-  2.0 fat    26663 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_regression.py
+-rw-rw-rw-  2.0 fat    42930 b- defN 23-May-28 20:48 sklearn/metrics/tests/test_score_objects.py
+-rw-rw-rw-  2.0 fat      253 b- defN 23-May-28 20:48 sklearn/mixture/__init__.py
+-rw-rw-rw-  2.0 fat    19403 b- defN 23-May-28 20:48 sklearn/mixture/_base.py
+-rw-rw-rw-  2.0 fat    34474 b- defN 23-May-28 20:48 sklearn/mixture/_bayesian_mixture.py
+-rw-rw-rw-  2.0 fat    30060 b- defN 23-May-28 20:48 sklearn/mixture/_gaussian_mixture.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/mixture/tests/__init__.py
+-rw-rw-rw-  2.0 fat    17676 b- defN 23-May-28 20:48 sklearn/mixture/tests/test_bayesian_mixture.py
+-rw-rw-rw-  2.0 fat    46301 b- defN 23-May-28 20:48 sklearn/mixture/tests/test_gaussian_mixture.py
+-rw-rw-rw-  2.0 fat     1050 b- defN 23-May-28 20:48 sklearn/mixture/tests/test_mixture.py
+-rw-rw-rw-  2.0 fat     2766 b- defN 23-May-28 20:48 sklearn/model_selection/__init__.py
+-rw-rw-rw-  2.0 fat    17518 b- defN 23-May-28 20:48 sklearn/model_selection/_plot.py
+-rw-rw-rw-  2.0 fat    72538 b- defN 23-May-28 20:48 sklearn/model_selection/_search.py
+-rw-rw-rw-  2.0 fat    45272 b- defN 23-May-28 20:48 sklearn/model_selection/_search_successive_halving.py
+-rw-rw-rw-  2.0 fat    97824 b- defN 23-May-28 20:48 sklearn/model_selection/_split.py
+-rw-rw-rw-  2.0 fat    73379 b- defN 23-May-28 20:48 sklearn/model_selection/_validation.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/model_selection/tests/__init__.py
+-rw-rw-rw-  2.0 fat      665 b- defN 23-May-28 20:48 sklearn/model_selection/tests/common.py
+-rw-rw-rw-  2.0 fat    11678 b- defN 23-May-28 20:48 sklearn/model_selection/tests/test_plot.py
+-rw-rw-rw-  2.0 fat    83149 b- defN 23-May-28 20:48 sklearn/model_selection/tests/test_search.py
+-rw-rw-rw-  2.0 fat    70844 b- defN 23-May-28 20:48 sklearn/model_selection/tests/test_split.py
+-rw-rw-rw-  2.0 fat    28426 b- defN 23-May-28 20:48 sklearn/model_selection/tests/test_successive_halving.py
+-rw-rw-rw-  2.0 fat    84852 b- defN 23-May-28 20:48 sklearn/model_selection/tests/test_validation.py
+-rw-rw-rw-  2.0 fat     1343 b- defN 23-May-28 20:48 sklearn/neighbors/__init__.py
+-rw-rw-rw-  2.0 fat    52381 b- defN 23-May-28 20:48 sklearn/neighbors/_base.py
+-rw-rw-rw-  2.0 fat    26900 b- defN 23-May-28 20:48 sklearn/neighbors/_classification.py
+-rw-rw-rw-  2.0 fat      599 b- defN 23-May-28 20:48 sklearn/neighbors/_distance_metric.py
+-rw-rw-rw-  2.0 fat    24060 b- defN 23-May-28 20:48 sklearn/neighbors/_graph.py
+-rw-rw-rw-  2.0 fat    12710 b- defN 23-May-28 20:48 sklearn/neighbors/_kde.py
+-rw-rw-rw-  2.0 fat    20141 b- defN 23-May-28 20:48 sklearn/neighbors/_lof.py
+-rw-rw-rw-  2.0 fat    20047 b- defN 23-May-28 20:48 sklearn/neighbors/_nca.py
+-rw-rw-rw-  2.0 fat     9029 b- defN 23-May-28 20:48 sklearn/neighbors/_nearest_centroid.py
+-rw-rw-rw-  2.0 fat    18130 b- defN 23-May-28 20:48 sklearn/neighbors/_regression.py
+-rw-rw-rw-  2.0 fat     6259 b- defN 23-May-28 20:48 sklearn/neighbors/_unsupervised.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/neighbors/tests/__init__.py
+-rw-rw-rw-  2.0 fat     3255 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_ball_tree.py
+-rw-rw-rw-  2.0 fat     3648 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_graph.py
+-rw-rw-rw-  2.0 fat     1063 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_kd_tree.py
+-rw-rw-rw-  2.0 fat     9999 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_kde.py
+-rw-rw-rw-  2.0 fat    11434 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_lof.py
+-rw-rw-rw-  2.0 fat    19598 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_nca.py
+-rw-rw-rw-  2.0 fat     4839 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_nearest_centroid.py
+-rw-rw-rw-  2.0 fat    81165 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_neighbors.py
+-rw-rw-rw-  2.0 fat     8574 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_neighbors_pipeline.py
+-rw-rw-rw-  2.0 fat     9511 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_neighbors_tree.py
+-rw-rw-rw-  2.0 fat     5000 b- defN 23-May-28 20:48 sklearn/neighbors/tests/test_quad_tree.py
+-rw-rw-rw-  2.0 fat      322 b- defN 23-May-28 20:48 sklearn/neural_network/__init__.py
+-rw-rw-rw-  2.0 fat     6567 b- defN 23-May-28 20:48 sklearn/neural_network/_base.py
+-rw-rw-rw-  2.0 fat    62267 b- defN 23-May-28 20:48 sklearn/neural_network/_multilayer_perceptron.py
+-rw-rw-rw-  2.0 fat    15329 b- defN 23-May-28 20:48 sklearn/neural_network/_rbm.py
+-rw-rw-rw-  2.0 fat     9111 b- defN 23-May-28 20:48 sklearn/neural_network/_stochastic_optimizers.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/neural_network/tests/__init__.py
+-rw-rw-rw-  2.0 fat      866 b- defN 23-May-28 20:48 sklearn/neural_network/tests/test_base.py
+-rw-rw-rw-  2.0 fat    31977 b- defN 23-May-28 20:48 sklearn/neural_network/tests/test_mlp.py
+-rw-rw-rw-  2.0 fat     8010 b- defN 23-May-28 20:48 sklearn/neural_network/tests/test_rbm.py
+-rw-rw-rw-  2.0 fat     4251 b- defN 23-May-28 20:48 sklearn/neural_network/tests/test_stochastic_optimizers.py
+-rw-rw-rw-  2.0 fat     1804 b- defN 23-May-28 20:48 sklearn/preprocessing/__init__.py
+-rw-rw-rw-  2.0 fat   123077 b- defN 23-May-28 20:48 sklearn/preprocessing/_data.py
+-rw-rw-rw-  2.0 fat    16116 b- defN 23-May-28 20:48 sklearn/preprocessing/_discretization.py
+-rw-rw-rw-  2.0 fat    58677 b- defN 23-May-28 20:48 sklearn/preprocessing/_encoders.py
+-rw-rw-rw-  2.0 fat    12986 b- defN 23-May-28 20:48 sklearn/preprocessing/_function_transformer.py
+-rw-rw-rw-  2.0 fat    31196 b- defN 23-May-28 20:48 sklearn/preprocessing/_label.py
+-rw-rw-rw-  2.0 fat    36957 b- defN 23-May-28 20:48 sklearn/preprocessing/_polynomial.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/__init__.py
+-rw-rw-rw-  2.0 fat     7180 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_common.py
+-rw-rw-rw-  2.0 fat    99981 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_data.py
+-rw-rw-rw-  2.0 fat    14286 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_discretization.py
+-rw-rw-rw-  2.0 fat    70340 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_encoders.py
+-rw-rw-rw-  2.0 fat    15441 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_function_transformer.py
+-rw-rw-rw-  2.0 fat    23375 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_label.py
+-rw-rw-rw-  2.0 fat    26608 b- defN 23-May-28 20:48 sklearn/preprocessing/tests/test_polynomial.py
+-rw-rw-rw-  2.0 fat      459 b- defN 23-May-28 20:48 sklearn/semi_supervised/__init__.py
+-rw-rw-rw-  2.0 fat    21776 b- defN 23-May-28 20:48 sklearn/semi_supervised/_label_propagation.py
+-rw-rw-rw-  2.0 fat    14195 b- defN 23-May-28 20:48 sklearn/semi_supervised/_self_training.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/semi_supervised/tests/__init__.py
+-rw-rw-rw-  2.0 fat     8032 b- defN 23-May-28 20:48 sklearn/semi_supervised/tests/test_label_propagation.py
+-rw-rw-rw-  2.0 fat    11702 b- defN 23-May-28 20:48 sklearn/semi_supervised/tests/test_self_training.py
+-rw-rw-rw-  2.0 fat      661 b- defN 23-May-28 20:48 sklearn/svm/__init__.py
+-rw-rw-rw-  2.0 fat    43760 b- defN 23-May-28 20:48 sklearn/svm/_base.py
+-rw-rw-rw-  2.0 fat     2996 b- defN 23-May-28 20:48 sklearn/svm/_bounds.py
+-rw-rw-rw-  2.0 fat    65767 b- defN 23-May-28 20:48 sklearn/svm/_classes.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/svm/tests/__init__.py
+-rw-rw-rw-  2.0 fat     4795 b- defN 23-May-28 20:48 sklearn/svm/tests/test_bounds.py
+-rw-rw-rw-  2.0 fat    16204 b- defN 23-May-28 20:48 sklearn/svm/tests/test_sparse.py
+-rw-rw-rw-  2.0 fat    48961 b- defN 23-May-28 20:48 sklearn/svm/tests/test_svm.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/tests/__init__.py
+-rw-rw-rw-  2.0 fat     3355 b- defN 23-May-28 20:48 sklearn/tests/random_seed.py
+-rw-rw-rw-  2.0 fat    22538 b- defN 23-May-28 20:48 sklearn/tests/test_base.py
+-rw-rw-rw-  2.0 fat     1213 b- defN 23-May-28 20:48 sklearn/tests/test_build.py
+-rw-rw-rw-  2.0 fat    40495 b- defN 23-May-28 20:48 sklearn/tests/test_calibration.py
+-rw-rw-rw-  2.0 fat      282 b- defN 23-May-28 20:48 sklearn/tests/test_check_build.py
+-rw-rw-rw-  2.0 fat    20479 b- defN 23-May-28 20:48 sklearn/tests/test_common.py
+-rw-rw-rw-  2.0 fat     5095 b- defN 23-May-28 20:48 sklearn/tests/test_config.py
+-rw-rw-rw-  2.0 fat    25857 b- defN 23-May-28 20:48 sklearn/tests/test_discriminant_analysis.py
+-rw-rw-rw-  2.0 fat    13148 b- defN 23-May-28 20:48 sklearn/tests/test_docstring_parameters.py
+-rw-rw-rw-  2.0 fat     7142 b- defN 23-May-28 20:48 sklearn/tests/test_docstrings.py
+-rw-rw-rw-  2.0 fat    21125 b- defN 23-May-28 20:48 sklearn/tests/test_dummy.py
+-rw-rw-rw-  2.0 fat      490 b- defN 23-May-28 20:48 sklearn/tests/test_init.py
+-rw-rw-rw-  2.0 fat    22875 b- defN 23-May-28 20:48 sklearn/tests/test_isotonic.py
+-rw-rw-rw-  2.0 fat    16772 b- defN 23-May-28 20:48 sklearn/tests/test_kernel_approximation.py
+-rw-rw-rw-  2.0 fat     3140 b- defN 23-May-28 20:48 sklearn/tests/test_kernel_ridge.py
+-rw-rw-rw-  2.0 fat    10656 b- defN 23-May-28 20:48 sklearn/tests/test_metaestimators.py
+-rw-rw-rw-  2.0 fat     3155 b- defN 23-May-28 20:48 sklearn/tests/test_min_dependencies_readme.py
+-rw-rw-rw-  2.0 fat    33227 b- defN 23-May-28 20:48 sklearn/tests/test_multiclass.py
+-rw-rw-rw-  2.0 fat    28012 b- defN 23-May-28 20:48 sklearn/tests/test_multioutput.py
+-rw-rw-rw-  2.0 fat    35719 b- defN 23-May-28 20:48 sklearn/tests/test_naive_bayes.py
+-rw-rw-rw-  2.0 fat    57299 b- defN 23-May-28 20:48 sklearn/tests/test_pipeline.py
+-rw-rw-rw-  2.0 fat     5416 b- defN 23-May-28 20:48 sklearn/tests/test_public_functions.py
+-rw-rw-rw-  2.0 fat    17155 b- defN 23-May-28 20:48 sklearn/tests/test_random_projection.py
+-rw-rw-rw-  2.0 fat      615 b- defN 23-May-28 20:48 sklearn/tree/__init__.py
+-rw-rw-rw-  2.0 fat    70801 b- defN 23-May-28 20:48 sklearn/tree/_classes.py
+-rw-rw-rw-  2.0 fat    37232 b- defN 23-May-28 20:48 sklearn/tree/_export.py
+-rw-rw-rw-  2.0 fat     5330 b- defN 23-May-28 20:48 sklearn/tree/_reingold_tilford.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/tree/tests/__init__.py
+-rw-rw-rw-  2.0 fat    17493 b- defN 23-May-28 20:48 sklearn/tree/tests/test_export.py
+-rw-rw-rw-  2.0 fat     1508 b- defN 23-May-28 20:48 sklearn/tree/tests/test_reingold_tilford.py
+-rw-rw-rw-  2.0 fat    85831 b- defN 23-May-28 20:48 sklearn/tree/tests/test_tree.py
+-rw-rw-rw-  2.0 fat    38375 b- defN 23-May-28 20:48 sklearn/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1159 b- defN 23-May-28 20:48 sklearn/utils/_arpack.py
+-rw-rw-rw-  2.0 fat     7988 b- defN 23-May-28 20:48 sklearn/utils/_array_api.py
+-rw-rw-rw-  2.0 fat     2792 b- defN 23-May-28 20:48 sklearn/utils/_available_if.py
+-rw-rw-rw-  2.0 fat     1439 b- defN 23-May-28 20:48 sklearn/utils/_bunch.py
+-rw-rw-rw-  2.0 fat    11733 b- defN 23-May-28 20:48 sklearn/utils/_encode.py
+-rw-rw-rw-  2.0 fat    12833 b- defN 23-May-28 20:48 sklearn/utils/_estimator_html_repr.py
+-rw-rw-rw-  2.0 fat      768 b- defN 23-May-28 20:48 sklearn/utils/_joblib.py
+-rw-rw-rw-  2.0 fat     1859 b- defN 23-May-28 20:48 sklearn/utils/_mask.py
+-rw-rw-rw-  2.0 fat    11460 b- defN 23-May-28 20:48 sklearn/utils/_mocking.py
+-rw-rw-rw-  2.0 fat    31022 b- defN 23-May-28 20:48 sklearn/utils/_param_validation.py
+-rw-rw-rw-  2.0 fat    18979 b- defN 23-May-28 20:48 sklearn/utils/_pprint.py
+-rw-rw-rw-  2.0 fat     9159 b- defN 23-May-28 20:48 sklearn/utils/_set_output.py
+-rw-rw-rw-  2.0 fat     2489 b- defN 23-May-28 20:48 sklearn/utils/_show_versions.py
+-rw-rw-rw-  2.0 fat     2106 b- defN 23-May-28 20:48 sklearn/utils/_tags.py
+-rw-rw-rw-  2.0 fat    35522 b- defN 23-May-28 20:48 sklearn/utils/_testing.py
+-rw-rw-rw-  2.0 fat     7503 b- defN 23-May-28 20:48 sklearn/utils/class_weight.py
+-rw-rw-rw-  2.0 fat     3371 b- defN 23-May-28 20:48 sklearn/utils/deprecation.py
+-rw-rw-rw-  2.0 fat     7555 b- defN 23-May-28 20:48 sklearn/utils/discovery.py
+-rw-rw-rw-  2.0 fat   159459 b- defN 23-May-28 20:48 sklearn/utils/estimator_checks.py
+-rw-rw-rw-  2.0 fat    40284 b- defN 23-May-28 20:48 sklearn/utils/extmath.py
+-rw-rw-rw-  2.0 fat     7235 b- defN 23-May-28 20:48 sklearn/utils/fixes.py
+-rw-rw-rw-  2.0 fat     5728 b- defN 23-May-28 20:48 sklearn/utils/graph.py
+-rw-rw-rw-  2.0 fat     8755 b- defN 23-May-28 20:48 sklearn/utils/metaestimators.py
+-rw-rw-rw-  2.0 fat    18960 b- defN 23-May-28 20:48 sklearn/utils/multiclass.py
+-rw-rw-rw-  2.0 fat     7739 b- defN 23-May-28 20:48 sklearn/utils/optimize.py
+-rw-rw-rw-  2.0 fat     4260 b- defN 23-May-28 20:48 sklearn/utils/parallel.py
+-rw-rw-rw-  2.0 fat     3667 b- defN 23-May-28 20:48 sklearn/utils/random.py
+-rw-rw-rw-  2.0 fat    19854 b- defN 23-May-28 20:48 sklearn/utils/sparsefuncs.py
+-rw-rw-rw-  2.0 fat     2426 b- defN 23-May-28 20:48 sklearn/utils/stats.py
+-rw-rw-rw-  2.0 fat    76952 b- defN 23-May-28 20:48 sklearn/utils/validation.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-28 20:48 sklearn/utils/tests/__init__.py
+-rw-rw-rw-  2.0 fat      217 b- defN 23-May-28 20:48 sklearn/utils/tests/conftest.py
+-rw-rw-rw-  2.0 fat      506 b- defN 23-May-28 20:48 sklearn/utils/tests/test_arpack.py
+-rw-rw-rw-  2.0 fat     6898 b- defN 23-May-28 20:48 sklearn/utils/tests/test_array_api.py
+-rw-rw-rw-  2.0 fat      819 b- defN 23-May-28 20:48 sklearn/utils/tests/test_arrayfuncs.py
+-rw-rw-rw-  2.0 fat    12327 b- defN 23-May-28 20:48 sklearn/utils/tests/test_class_weight.py
+-rw-rw-rw-  2.0 fat     7082 b- defN 23-May-28 20:48 sklearn/utils/tests/test_cython_blas.py
+-rw-rw-rw-  2.0 fat      852 b- defN 23-May-28 20:48 sklearn/utils/tests/test_cython_templating.py
+-rw-rw-rw-  2.0 fat     1566 b- defN 23-May-28 20:48 sklearn/utils/tests/test_deprecation.py
+-rw-rw-rw-  2.0 fat     9979 b- defN 23-May-28 20:48 sklearn/utils/tests/test_encode.py
+-rw-rw-rw-  2.0 fat    43418 b- defN 23-May-28 20:48 sklearn/utils/tests/test_estimator_checks.py
+-rw-rw-rw-  2.0 fat    11815 b- defN 23-May-28 20:48 sklearn/utils/tests/test_estimator_html_repr.py
+-rw-rw-rw-  2.0 fat    37650 b- defN 23-May-28 20:48 sklearn/utils/tests/test_extmath.py
+-rw-rw-rw-  2.0 fat      847 b- defN 23-May-28 20:48 sklearn/utils/tests/test_fast_dict.py
+-rw-rw-rw-  2.0 fat     1858 b- defN 23-May-28 20:48 sklearn/utils/tests/test_fixes.py
+-rw-rw-rw-  2.0 fat     3127 b- defN 23-May-28 20:48 sklearn/utils/tests/test_graph.py
+-rw-rw-rw-  2.0 fat     5546 b- defN 23-May-28 20:48 sklearn/utils/tests/test_metaestimators.py
+-rw-rw-rw-  2.0 fat     5305 b- defN 23-May-28 20:48 sklearn/utils/tests/test_mocking.py
+-rw-rw-rw-  2.0 fat    17606 b- defN 23-May-28 20:48 sklearn/utils/tests/test_multiclass.py
+-rw-rw-rw-  2.0 fat     2613 b- defN 23-May-28 20:48 sklearn/utils/tests/test_murmurhash.py
+-rw-rw-rw-  2.0 fat      801 b- defN 23-May-28 20:48 sklearn/utils/tests/test_optimize.py
+-rw-rw-rw-  2.0 fat     3752 b- defN 23-May-28 20:48 sklearn/utils/tests/test_parallel.py
+-rw-rw-rw-  2.0 fat    22335 b- defN 23-May-28 20:48 sklearn/utils/tests/test_param_validation.py
+-rw-rw-rw-  2.0 fat    28021 b- defN 23-May-28 20:48 sklearn/utils/tests/test_pprint.py
+-rw-rw-rw-  2.0 fat     7349 b- defN 23-May-28 20:48 sklearn/utils/tests/test_random.py
+-rw-rw-rw-  2.0 fat     1385 b- defN 23-May-28 20:48 sklearn/utils/tests/test_readonly_wrapper.py
+-rw-rw-rw-  2.0 fat     5337 b- defN 23-May-28 20:48 sklearn/utils/tests/test_seq_dataset.py
+-rw-rw-rw-  2.0 fat     9689 b- defN 23-May-28 20:48 sklearn/utils/tests/test_set_output.py
+-rw-rw-rw-  2.0 fat     1909 b- defN 23-May-28 20:48 sklearn/utils/tests/test_shortest_path.py
+-rw-rw-rw-  2.0 fat     1127 b- defN 23-May-28 20:48 sklearn/utils/tests/test_show_versions.py
+-rw-rw-rw-  2.0 fat    31808 b- defN 23-May-28 20:48 sklearn/utils/tests/test_sparsefuncs.py
+-rw-rw-rw-  2.0 fat     2858 b- defN 23-May-28 20:48 sklearn/utils/tests/test_stats.py
+-rw-rw-rw-  2.0 fat     1443 b- defN 23-May-28 20:48 sklearn/utils/tests/test_tags.py
+-rw-rw-rw-  2.0 fat    25412 b- defN 23-May-28 20:48 sklearn/utils/tests/test_testing.py
+-rw-rw-rw-  2.0 fat    27674 b- defN 23-May-28 20:48 sklearn/utils/tests/test_utils.py
+-rw-rw-rw-  2.0 fat    61538 b- defN 23-May-28 20:48 sklearn/utils/tests/test_validation.py
+-rw-rw-rw-  2.0 fat      688 b- defN 23-May-28 20:48 sklearn/utils/tests/test_weight_vector.py
+-rw-rw-rw-  2.0 fat     1546 b- defN 23-May-28 20:55 pdc_dp_means-0.0.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     6027 b- defN 23-May-28 20:55 pdc_dp_means-0.0.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-28 20:55 pdc_dp_means-0.0.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       21 b- defN 23-May-28 20:55 pdc_dp_means-0.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    51900 b- defN 23-May-28 20:55 pdc_dp_means-0.0.4.dist-info/RECORD
+547 files, 10941409 bytes uncompressed, 2464639 bytes compressed:  77.5%
```

## zipnote {}

```diff
@@ -1,28 +1,1642 @@
 Filename: pdc_dp_means/__init__.py
 Comment: 
 
-Filename: pdc_dp_means/dp_means_cython.cpython-38-x86_64-linux-gnu.so
+Filename: pdc_dp_means/dp_means_cython.cp39-win_amd64.pyd
 Comment: 
 
 Filename: pdc_dp_means/dpmeans.py
 Comment: 
 
 Filename: pdc_dp_means/release.py
 Comment: 
 
-Filename: pdc_dp_means-0.0.3.dist-info/LICENSE
+Filename: sklearn/__init__.py
 Comment: 
 
-Filename: pdc_dp_means-0.0.3.dist-info/METADATA
+Filename: sklearn/_config.py
 Comment: 
 
-Filename: pdc_dp_means-0.0.3.dist-info/WHEEL
+Filename: sklearn/_distributor_init.py
 Comment: 
 
-Filename: pdc_dp_means-0.0.3.dist-info/top_level.txt
+Filename: sklearn/_min_dependencies.py
 Comment: 
 
-Filename: pdc_dp_means-0.0.3.dist-info/RECORD
+Filename: sklearn/base.py
+Comment: 
+
+Filename: sklearn/calibration.py
+Comment: 
+
+Filename: sklearn/conftest.py
+Comment: 
+
+Filename: sklearn/discriminant_analysis.py
+Comment: 
+
+Filename: sklearn/dummy.py
+Comment: 
+
+Filename: sklearn/exceptions.py
+Comment: 
+
+Filename: sklearn/isotonic.py
+Comment: 
+
+Filename: sklearn/kernel_approximation.py
+Comment: 
+
+Filename: sklearn/kernel_ridge.py
+Comment: 
+
+Filename: sklearn/multiclass.py
+Comment: 
+
+Filename: sklearn/multioutput.py
+Comment: 
+
+Filename: sklearn/naive_bayes.py
+Comment: 
+
+Filename: sklearn/pipeline.py
+Comment: 
+
+Filename: sklearn/random_projection.py
+Comment: 
+
+Filename: sklearn/__check_build/__init__.py
+Comment: 
+
+Filename: sklearn/_build_utils/__init__.py
+Comment: 
+
+Filename: sklearn/_build_utils/openmp_helpers.py
+Comment: 
+
+Filename: sklearn/_build_utils/pre_build_helpers.py
+Comment: 
+
+Filename: sklearn/_loss/__init__.py
+Comment: 
+
+Filename: sklearn/_loss/glm_distribution.py
+Comment: 
+
+Filename: sklearn/_loss/link.py
+Comment: 
+
+Filename: sklearn/_loss/loss.py
+Comment: 
+
+Filename: sklearn/_loss/tests/__init__.py
+Comment: 
+
+Filename: sklearn/_loss/tests/test_glm_distribution.py
+Comment: 
+
+Filename: sklearn/_loss/tests/test_link.py
+Comment: 
+
+Filename: sklearn/_loss/tests/test_loss.py
+Comment: 
+
+Filename: sklearn/cluster/__init__.py
+Comment: 
+
+Filename: sklearn/cluster/_affinity_propagation.py
+Comment: 
+
+Filename: sklearn/cluster/_agglomerative.py
+Comment: 
+
+Filename: sklearn/cluster/_bicluster.py
+Comment: 
+
+Filename: sklearn/cluster/_birch.py
+Comment: 
+
+Filename: sklearn/cluster/_bisect_k_means.py
+Comment: 
+
+Filename: sklearn/cluster/_dbscan.py
+Comment: 
+
+Filename: sklearn/cluster/_feature_agglomeration.py
+Comment: 
+
+Filename: sklearn/cluster/_kmeans.py
+Comment: 
+
+Filename: sklearn/cluster/_mean_shift.py
+Comment: 
+
+Filename: sklearn/cluster/_optics.py
+Comment: 
+
+Filename: sklearn/cluster/_spectral.py
+Comment: 
+
+Filename: sklearn/cluster/tests/__init__.py
+Comment: 
+
+Filename: sklearn/cluster/tests/common.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_affinity_propagation.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_bicluster.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_birch.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_bisect_k_means.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_dbscan.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_feature_agglomeration.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_hierarchical.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_k_means.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_mean_shift.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_optics.py
+Comment: 
+
+Filename: sklearn/cluster/tests/test_spectral.py
+Comment: 
+
+Filename: sklearn/compose/__init__.py
+Comment: 
+
+Filename: sklearn/compose/_column_transformer.py
+Comment: 
+
+Filename: sklearn/compose/_target.py
+Comment: 
+
+Filename: sklearn/compose/tests/__init__.py
+Comment: 
+
+Filename: sklearn/compose/tests/test_column_transformer.py
+Comment: 
+
+Filename: sklearn/compose/tests/test_target.py
+Comment: 
+
+Filename: sklearn/covariance/__init__.py
+Comment: 
+
+Filename: sklearn/covariance/_elliptic_envelope.py
+Comment: 
+
+Filename: sklearn/covariance/_empirical_covariance.py
+Comment: 
+
+Filename: sklearn/covariance/_graph_lasso.py
+Comment: 
+
+Filename: sklearn/covariance/_robust_covariance.py
+Comment: 
+
+Filename: sklearn/covariance/_shrunk_covariance.py
+Comment: 
+
+Filename: sklearn/covariance/tests/__init__.py
+Comment: 
+
+Filename: sklearn/covariance/tests/test_covariance.py
+Comment: 
+
+Filename: sklearn/covariance/tests/test_elliptic_envelope.py
+Comment: 
+
+Filename: sklearn/covariance/tests/test_graphical_lasso.py
+Comment: 
+
+Filename: sklearn/covariance/tests/test_robust_covariance.py
+Comment: 
+
+Filename: sklearn/cross_decomposition/__init__.py
+Comment: 
+
+Filename: sklearn/cross_decomposition/_pls.py
+Comment: 
+
+Filename: sklearn/cross_decomposition/tests/__init__.py
+Comment: 
+
+Filename: sklearn/cross_decomposition/tests/test_pls.py
+Comment: 
+
+Filename: sklearn/datasets/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/_arff_parser.py
+Comment: 
+
+Filename: sklearn/datasets/_base.py
+Comment: 
+
+Filename: sklearn/datasets/_california_housing.py
+Comment: 
+
+Filename: sklearn/datasets/_covtype.py
+Comment: 
+
+Filename: sklearn/datasets/_kddcup99.py
+Comment: 
+
+Filename: sklearn/datasets/_lfw.py
+Comment: 
+
+Filename: sklearn/datasets/_olivetti_faces.py
+Comment: 
+
+Filename: sklearn/datasets/_openml.py
+Comment: 
+
+Filename: sklearn/datasets/_rcv1.py
+Comment: 
+
+Filename: sklearn/datasets/_samples_generator.py
+Comment: 
+
+Filename: sklearn/datasets/_species_distributions.py
+Comment: 
+
+Filename: sklearn/datasets/_svmlight_format_io.py
+Comment: 
+
+Filename: sklearn/datasets/_twenty_newsgroups.py
+Comment: 
+
+Filename: sklearn/datasets/data/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/descr/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/images/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/conftest.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_20news.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_arff_parser.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_base.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_california_housing.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_common.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_covtype.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_kddcup99.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_lfw.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_olivetti_faces.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_openml.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_rcv1.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_samples_generator.py
+Comment: 
+
+Filename: sklearn/datasets/tests/test_svmlight_format.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_1/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_1119/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_1590/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_2/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_292/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_3/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_40589/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_40675/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_40945/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_40966/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_42074/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_42585/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_561/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_61/__init__.py
+Comment: 
+
+Filename: sklearn/datasets/tests/data/openml/id_62/__init__.py
+Comment: 
+
+Filename: sklearn/decomposition/__init__.py
+Comment: 
+
+Filename: sklearn/decomposition/_base.py
+Comment: 
+
+Filename: sklearn/decomposition/_dict_learning.py
+Comment: 
+
+Filename: sklearn/decomposition/_factor_analysis.py
+Comment: 
+
+Filename: sklearn/decomposition/_fastica.py
+Comment: 
+
+Filename: sklearn/decomposition/_incremental_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/_kernel_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/_lda.py
+Comment: 
+
+Filename: sklearn/decomposition/_nmf.py
+Comment: 
+
+Filename: sklearn/decomposition/_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/_sparse_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/_truncated_svd.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/__init__.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_dict_learning.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_factor_analysis.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_fastica.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_incremental_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_kernel_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_nmf.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_online_lda.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_sparse_pca.py
+Comment: 
+
+Filename: sklearn/decomposition/tests/test_truncated_svd.py
+Comment: 
+
+Filename: sklearn/ensemble/__init__.py
+Comment: 
+
+Filename: sklearn/ensemble/_bagging.py
+Comment: 
+
+Filename: sklearn/ensemble/_base.py
+Comment: 
+
+Filename: sklearn/ensemble/_forest.py
+Comment: 
+
+Filename: sklearn/ensemble/_gb.py
+Comment: 
+
+Filename: sklearn/ensemble/_gb_losses.py
+Comment: 
+
+Filename: sklearn/ensemble/_iforest.py
+Comment: 
+
+Filename: sklearn/ensemble/_stacking.py
+Comment: 
+
+Filename: sklearn/ensemble/_voting.py
+Comment: 
+
+Filename: sklearn/ensemble/_weight_boosting.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/__init__.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/binning.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/grower.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/predictor.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/__init__.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py
+Comment: 
+
+Filename: sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/__init__.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_bagging.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_base.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_common.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_forest.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_gradient_boosting.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_iforest.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_stacking.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_voting.py
+Comment: 
+
+Filename: sklearn/ensemble/tests/test_weight_boosting.py
+Comment: 
+
+Filename: sklearn/experimental/__init__.py
+Comment: 
+
+Filename: sklearn/experimental/enable_halving_search_cv.py
+Comment: 
+
+Filename: sklearn/experimental/enable_hist_gradient_boosting.py
+Comment: 
+
+Filename: sklearn/experimental/enable_iterative_imputer.py
+Comment: 
+
+Filename: sklearn/experimental/tests/__init__.py
+Comment: 
+
+Filename: sklearn/experimental/tests/test_enable_hist_gradient_boosting.py
+Comment: 
+
+Filename: sklearn/experimental/tests/test_enable_iterative_imputer.py
+Comment: 
+
+Filename: sklearn/experimental/tests/test_enable_successive_halving.py
+Comment: 
+
+Filename: sklearn/externals/__init__.py
+Comment: 
+
+Filename: sklearn/externals/_arff.py
+Comment: 
+
+Filename: sklearn/externals/_lobpcg.py
+Comment: 
+
+Filename: sklearn/externals/conftest.py
+Comment: 
+
+Filename: sklearn/externals/_packaging/__init__.py
+Comment: 
+
+Filename: sklearn/externals/_packaging/_structures.py
+Comment: 
+
+Filename: sklearn/externals/_packaging/version.py
+Comment: 
+
+Filename: sklearn/feature_extraction/__init__.py
+Comment: 
+
+Filename: sklearn/feature_extraction/_dict_vectorizer.py
+Comment: 
+
+Filename: sklearn/feature_extraction/_hash.py
+Comment: 
+
+Filename: sklearn/feature_extraction/_stop_words.py
+Comment: 
+
+Filename: sklearn/feature_extraction/image.py
+Comment: 
+
+Filename: sklearn/feature_extraction/text.py
+Comment: 
+
+Filename: sklearn/feature_extraction/tests/__init__.py
+Comment: 
+
+Filename: sklearn/feature_extraction/tests/test_dict_vectorizer.py
+Comment: 
+
+Filename: sklearn/feature_extraction/tests/test_feature_hasher.py
+Comment: 
+
+Filename: sklearn/feature_extraction/tests/test_image.py
+Comment: 
+
+Filename: sklearn/feature_extraction/tests/test_text.py
+Comment: 
+
+Filename: sklearn/feature_selection/__init__.py
+Comment: 
+
+Filename: sklearn/feature_selection/_base.py
+Comment: 
+
+Filename: sklearn/feature_selection/_from_model.py
+Comment: 
+
+Filename: sklearn/feature_selection/_mutual_info.py
+Comment: 
+
+Filename: sklearn/feature_selection/_rfe.py
+Comment: 
+
+Filename: sklearn/feature_selection/_sequential.py
+Comment: 
+
+Filename: sklearn/feature_selection/_univariate_selection.py
+Comment: 
+
+Filename: sklearn/feature_selection/_variance_threshold.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/__init__.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_base.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_chi2.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_feature_select.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_from_model.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_mutual_info.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_rfe.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_sequential.py
+Comment: 
+
+Filename: sklearn/feature_selection/tests/test_variance_threshold.py
+Comment: 
+
+Filename: sklearn/gaussian_process/__init__.py
+Comment: 
+
+Filename: sklearn/gaussian_process/_gpc.py
+Comment: 
+
+Filename: sklearn/gaussian_process/_gpr.py
+Comment: 
+
+Filename: sklearn/gaussian_process/kernels.py
+Comment: 
+
+Filename: sklearn/gaussian_process/tests/__init__.py
+Comment: 
+
+Filename: sklearn/gaussian_process/tests/_mini_sequence_kernel.py
+Comment: 
+
+Filename: sklearn/gaussian_process/tests/test_gpc.py
+Comment: 
+
+Filename: sklearn/gaussian_process/tests/test_gpr.py
+Comment: 
+
+Filename: sklearn/gaussian_process/tests/test_kernels.py
+Comment: 
+
+Filename: sklearn/impute/__init__.py
+Comment: 
+
+Filename: sklearn/impute/_base.py
+Comment: 
+
+Filename: sklearn/impute/_iterative.py
+Comment: 
+
+Filename: sklearn/impute/_knn.py
+Comment: 
+
+Filename: sklearn/impute/tests/__init__.py
+Comment: 
+
+Filename: sklearn/impute/tests/test_base.py
+Comment: 
+
+Filename: sklearn/impute/tests/test_common.py
+Comment: 
+
+Filename: sklearn/impute/tests/test_impute.py
+Comment: 
+
+Filename: sklearn/impute/tests/test_knn.py
+Comment: 
+
+Filename: sklearn/inspection/__init__.py
+Comment: 
+
+Filename: sklearn/inspection/_partial_dependence.py
+Comment: 
+
+Filename: sklearn/inspection/_pd_utils.py
+Comment: 
+
+Filename: sklearn/inspection/_permutation_importance.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/__init__.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/decision_boundary.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/partial_dependence.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/tests/__init__.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/tests/test_boundary_decision_display.py
+Comment: 
+
+Filename: sklearn/inspection/_plot/tests/test_plot_partial_dependence.py
+Comment: 
+
+Filename: sklearn/inspection/tests/__init__.py
+Comment: 
+
+Filename: sklearn/inspection/tests/test_partial_dependence.py
+Comment: 
+
+Filename: sklearn/inspection/tests/test_pd_utils.py
+Comment: 
+
+Filename: sklearn/inspection/tests/test_permutation_importance.py
+Comment: 
+
+Filename: sklearn/linear_model/__init__.py
+Comment: 
+
+Filename: sklearn/linear_model/_base.py
+Comment: 
+
+Filename: sklearn/linear_model/_bayes.py
+Comment: 
+
+Filename: sklearn/linear_model/_coordinate_descent.py
+Comment: 
+
+Filename: sklearn/linear_model/_huber.py
+Comment: 
+
+Filename: sklearn/linear_model/_least_angle.py
+Comment: 
+
+Filename: sklearn/linear_model/_linear_loss.py
+Comment: 
+
+Filename: sklearn/linear_model/_logistic.py
+Comment: 
+
+Filename: sklearn/linear_model/_omp.py
+Comment: 
+
+Filename: sklearn/linear_model/_passive_aggressive.py
+Comment: 
+
+Filename: sklearn/linear_model/_perceptron.py
+Comment: 
+
+Filename: sklearn/linear_model/_quantile.py
+Comment: 
+
+Filename: sklearn/linear_model/_ransac.py
+Comment: 
+
+Filename: sklearn/linear_model/_ridge.py
+Comment: 
+
+Filename: sklearn/linear_model/_sag.py
+Comment: 
+
+Filename: sklearn/linear_model/_stochastic_gradient.py
+Comment: 
+
+Filename: sklearn/linear_model/_theil_sen.py
+Comment: 
+
+Filename: sklearn/linear_model/_glm/__init__.py
+Comment: 
+
+Filename: sklearn/linear_model/_glm/_newton_solver.py
+Comment: 
+
+Filename: sklearn/linear_model/_glm/glm.py
+Comment: 
+
+Filename: sklearn/linear_model/_glm/tests/__init__.py
+Comment: 
+
+Filename: sklearn/linear_model/_glm/tests/test_glm.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/__init__.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_base.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_bayes.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_common.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_coordinate_descent.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_huber.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_least_angle.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_linear_loss.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_logistic.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_omp.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_passive_aggressive.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_perceptron.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_quantile.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_ransac.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_ridge.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_sag.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_sgd.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_sparse_coordinate_descent.py
+Comment: 
+
+Filename: sklearn/linear_model/tests/test_theil_sen.py
+Comment: 
+
+Filename: sklearn/manifold/__init__.py
+Comment: 
+
+Filename: sklearn/manifold/_isomap.py
+Comment: 
+
+Filename: sklearn/manifold/_locally_linear.py
+Comment: 
+
+Filename: sklearn/manifold/_mds.py
+Comment: 
+
+Filename: sklearn/manifold/_spectral_embedding.py
+Comment: 
+
+Filename: sklearn/manifold/_t_sne.py
+Comment: 
+
+Filename: sklearn/manifold/tests/__init__.py
+Comment: 
+
+Filename: sklearn/manifold/tests/test_isomap.py
+Comment: 
+
+Filename: sklearn/manifold/tests/test_locally_linear.py
+Comment: 
+
+Filename: sklearn/manifold/tests/test_mds.py
+Comment: 
+
+Filename: sklearn/manifold/tests/test_spectral_embedding.py
+Comment: 
+
+Filename: sklearn/manifold/tests/test_t_sne.py
+Comment: 
+
+Filename: sklearn/metrics/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/_base.py
+Comment: 
+
+Filename: sklearn/metrics/_classification.py
+Comment: 
+
+Filename: sklearn/metrics/_ranking.py
+Comment: 
+
+Filename: sklearn/metrics/_regression.py
+Comment: 
+
+Filename: sklearn/metrics/_scorer.py
+Comment: 
+
+Filename: sklearn/metrics/pairwise.py
+Comment: 
+
+Filename: sklearn/metrics/_pairwise_distances_reduction/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/base.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/confusion_matrix.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/det_curve.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/precision_recall_curve.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/regression.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/roc_curve.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_base.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_common_curve_display.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_confusion_matrix_display.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_det_curve_display.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_precision_recall_display.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_predict_error_display.py
+Comment: 
+
+Filename: sklearn/metrics/_plot/tests/test_roc_curve_display.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/_bicluster.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/_supervised.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/_unsupervised.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/tests/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/tests/test_bicluster.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/tests/test_common.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/tests/test_supervised.py
+Comment: 
+
+Filename: sklearn/metrics/cluster/tests/test_unsupervised.py
+Comment: 
+
+Filename: sklearn/metrics/tests/__init__.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_classification.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_common.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_dist_metrics.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_pairwise.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_pairwise_distances_reduction.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_ranking.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_regression.py
+Comment: 
+
+Filename: sklearn/metrics/tests/test_score_objects.py
+Comment: 
+
+Filename: sklearn/mixture/__init__.py
+Comment: 
+
+Filename: sklearn/mixture/_base.py
+Comment: 
+
+Filename: sklearn/mixture/_bayesian_mixture.py
+Comment: 
+
+Filename: sklearn/mixture/_gaussian_mixture.py
+Comment: 
+
+Filename: sklearn/mixture/tests/__init__.py
+Comment: 
+
+Filename: sklearn/mixture/tests/test_bayesian_mixture.py
+Comment: 
+
+Filename: sklearn/mixture/tests/test_gaussian_mixture.py
+Comment: 
+
+Filename: sklearn/mixture/tests/test_mixture.py
+Comment: 
+
+Filename: sklearn/model_selection/__init__.py
+Comment: 
+
+Filename: sklearn/model_selection/_plot.py
+Comment: 
+
+Filename: sklearn/model_selection/_search.py
+Comment: 
+
+Filename: sklearn/model_selection/_search_successive_halving.py
+Comment: 
+
+Filename: sklearn/model_selection/_split.py
+Comment: 
+
+Filename: sklearn/model_selection/_validation.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/__init__.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/common.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/test_plot.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/test_search.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/test_split.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/test_successive_halving.py
+Comment: 
+
+Filename: sklearn/model_selection/tests/test_validation.py
+Comment: 
+
+Filename: sklearn/neighbors/__init__.py
+Comment: 
+
+Filename: sklearn/neighbors/_base.py
+Comment: 
+
+Filename: sklearn/neighbors/_classification.py
+Comment: 
+
+Filename: sklearn/neighbors/_distance_metric.py
+Comment: 
+
+Filename: sklearn/neighbors/_graph.py
+Comment: 
+
+Filename: sklearn/neighbors/_kde.py
+Comment: 
+
+Filename: sklearn/neighbors/_lof.py
+Comment: 
+
+Filename: sklearn/neighbors/_nca.py
+Comment: 
+
+Filename: sklearn/neighbors/_nearest_centroid.py
+Comment: 
+
+Filename: sklearn/neighbors/_regression.py
+Comment: 
+
+Filename: sklearn/neighbors/_unsupervised.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/__init__.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_ball_tree.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_graph.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_kd_tree.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_kde.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_lof.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_nca.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_nearest_centroid.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_neighbors.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_neighbors_pipeline.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_neighbors_tree.py
+Comment: 
+
+Filename: sklearn/neighbors/tests/test_quad_tree.py
+Comment: 
+
+Filename: sklearn/neural_network/__init__.py
+Comment: 
+
+Filename: sklearn/neural_network/_base.py
+Comment: 
+
+Filename: sklearn/neural_network/_multilayer_perceptron.py
+Comment: 
+
+Filename: sklearn/neural_network/_rbm.py
+Comment: 
+
+Filename: sklearn/neural_network/_stochastic_optimizers.py
+Comment: 
+
+Filename: sklearn/neural_network/tests/__init__.py
+Comment: 
+
+Filename: sklearn/neural_network/tests/test_base.py
+Comment: 
+
+Filename: sklearn/neural_network/tests/test_mlp.py
+Comment: 
+
+Filename: sklearn/neural_network/tests/test_rbm.py
+Comment: 
+
+Filename: sklearn/neural_network/tests/test_stochastic_optimizers.py
+Comment: 
+
+Filename: sklearn/preprocessing/__init__.py
+Comment: 
+
+Filename: sklearn/preprocessing/_data.py
+Comment: 
+
+Filename: sklearn/preprocessing/_discretization.py
+Comment: 
+
+Filename: sklearn/preprocessing/_encoders.py
+Comment: 
+
+Filename: sklearn/preprocessing/_function_transformer.py
+Comment: 
+
+Filename: sklearn/preprocessing/_label.py
+Comment: 
+
+Filename: sklearn/preprocessing/_polynomial.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/__init__.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_common.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_data.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_discretization.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_encoders.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_function_transformer.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_label.py
+Comment: 
+
+Filename: sklearn/preprocessing/tests/test_polynomial.py
+Comment: 
+
+Filename: sklearn/semi_supervised/__init__.py
+Comment: 
+
+Filename: sklearn/semi_supervised/_label_propagation.py
+Comment: 
+
+Filename: sklearn/semi_supervised/_self_training.py
+Comment: 
+
+Filename: sklearn/semi_supervised/tests/__init__.py
+Comment: 
+
+Filename: sklearn/semi_supervised/tests/test_label_propagation.py
+Comment: 
+
+Filename: sklearn/semi_supervised/tests/test_self_training.py
+Comment: 
+
+Filename: sklearn/svm/__init__.py
+Comment: 
+
+Filename: sklearn/svm/_base.py
+Comment: 
+
+Filename: sklearn/svm/_bounds.py
+Comment: 
+
+Filename: sklearn/svm/_classes.py
+Comment: 
+
+Filename: sklearn/svm/tests/__init__.py
+Comment: 
+
+Filename: sklearn/svm/tests/test_bounds.py
+Comment: 
+
+Filename: sklearn/svm/tests/test_sparse.py
+Comment: 
+
+Filename: sklearn/svm/tests/test_svm.py
+Comment: 
+
+Filename: sklearn/tests/__init__.py
+Comment: 
+
+Filename: sklearn/tests/random_seed.py
+Comment: 
+
+Filename: sklearn/tests/test_base.py
+Comment: 
+
+Filename: sklearn/tests/test_build.py
+Comment: 
+
+Filename: sklearn/tests/test_calibration.py
+Comment: 
+
+Filename: sklearn/tests/test_check_build.py
+Comment: 
+
+Filename: sklearn/tests/test_common.py
+Comment: 
+
+Filename: sklearn/tests/test_config.py
+Comment: 
+
+Filename: sklearn/tests/test_discriminant_analysis.py
+Comment: 
+
+Filename: sklearn/tests/test_docstring_parameters.py
+Comment: 
+
+Filename: sklearn/tests/test_docstrings.py
+Comment: 
+
+Filename: sklearn/tests/test_dummy.py
+Comment: 
+
+Filename: sklearn/tests/test_init.py
+Comment: 
+
+Filename: sklearn/tests/test_isotonic.py
+Comment: 
+
+Filename: sklearn/tests/test_kernel_approximation.py
+Comment: 
+
+Filename: sklearn/tests/test_kernel_ridge.py
+Comment: 
+
+Filename: sklearn/tests/test_metaestimators.py
+Comment: 
+
+Filename: sklearn/tests/test_min_dependencies_readme.py
+Comment: 
+
+Filename: sklearn/tests/test_multiclass.py
+Comment: 
+
+Filename: sklearn/tests/test_multioutput.py
+Comment: 
+
+Filename: sklearn/tests/test_naive_bayes.py
+Comment: 
+
+Filename: sklearn/tests/test_pipeline.py
+Comment: 
+
+Filename: sklearn/tests/test_public_functions.py
+Comment: 
+
+Filename: sklearn/tests/test_random_projection.py
+Comment: 
+
+Filename: sklearn/tree/__init__.py
+Comment: 
+
+Filename: sklearn/tree/_classes.py
+Comment: 
+
+Filename: sklearn/tree/_export.py
+Comment: 
+
+Filename: sklearn/tree/_reingold_tilford.py
+Comment: 
+
+Filename: sklearn/tree/tests/__init__.py
+Comment: 
+
+Filename: sklearn/tree/tests/test_export.py
+Comment: 
+
+Filename: sklearn/tree/tests/test_reingold_tilford.py
+Comment: 
+
+Filename: sklearn/tree/tests/test_tree.py
+Comment: 
+
+Filename: sklearn/utils/__init__.py
+Comment: 
+
+Filename: sklearn/utils/_arpack.py
+Comment: 
+
+Filename: sklearn/utils/_array_api.py
+Comment: 
+
+Filename: sklearn/utils/_available_if.py
+Comment: 
+
+Filename: sklearn/utils/_bunch.py
+Comment: 
+
+Filename: sklearn/utils/_encode.py
+Comment: 
+
+Filename: sklearn/utils/_estimator_html_repr.py
+Comment: 
+
+Filename: sklearn/utils/_joblib.py
+Comment: 
+
+Filename: sklearn/utils/_mask.py
+Comment: 
+
+Filename: sklearn/utils/_mocking.py
+Comment: 
+
+Filename: sklearn/utils/_param_validation.py
+Comment: 
+
+Filename: sklearn/utils/_pprint.py
+Comment: 
+
+Filename: sklearn/utils/_set_output.py
+Comment: 
+
+Filename: sklearn/utils/_show_versions.py
+Comment: 
+
+Filename: sklearn/utils/_tags.py
+Comment: 
+
+Filename: sklearn/utils/_testing.py
+Comment: 
+
+Filename: sklearn/utils/class_weight.py
+Comment: 
+
+Filename: sklearn/utils/deprecation.py
+Comment: 
+
+Filename: sklearn/utils/discovery.py
+Comment: 
+
+Filename: sklearn/utils/estimator_checks.py
+Comment: 
+
+Filename: sklearn/utils/extmath.py
+Comment: 
+
+Filename: sklearn/utils/fixes.py
+Comment: 
+
+Filename: sklearn/utils/graph.py
+Comment: 
+
+Filename: sklearn/utils/metaestimators.py
+Comment: 
+
+Filename: sklearn/utils/multiclass.py
+Comment: 
+
+Filename: sklearn/utils/optimize.py
+Comment: 
+
+Filename: sklearn/utils/parallel.py
+Comment: 
+
+Filename: sklearn/utils/random.py
+Comment: 
+
+Filename: sklearn/utils/sparsefuncs.py
+Comment: 
+
+Filename: sklearn/utils/stats.py
+Comment: 
+
+Filename: sklearn/utils/validation.py
+Comment: 
+
+Filename: sklearn/utils/tests/__init__.py
+Comment: 
+
+Filename: sklearn/utils/tests/conftest.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_arpack.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_array_api.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_arrayfuncs.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_class_weight.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_cython_blas.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_cython_templating.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_deprecation.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_encode.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_estimator_checks.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_estimator_html_repr.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_extmath.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_fast_dict.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_fixes.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_graph.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_metaestimators.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_mocking.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_multiclass.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_murmurhash.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_optimize.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_parallel.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_param_validation.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_pprint.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_random.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_readonly_wrapper.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_seq_dataset.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_set_output.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_shortest_path.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_show_versions.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_sparsefuncs.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_stats.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_tags.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_testing.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_utils.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_validation.py
+Comment: 
+
+Filename: sklearn/utils/tests/test_weight_vector.py
+Comment: 
+
+Filename: pdc_dp_means-0.0.4.dist-info/LICENSE
+Comment: 
+
+Filename: pdc_dp_means-0.0.4.dist-info/METADATA
+Comment: 
+
+Filename: pdc_dp_means-0.0.4.dist-info/WHEEL
+Comment: 
+
+Filename: pdc_dp_means-0.0.4.dist-info/top_level.txt
+Comment: 
+
+Filename: pdc_dp_means-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pdc_dp_means/__init__.py

```diff
@@ -1,4 +1 @@
-from .dpmeans import (
-    MiniBatchDPMeans,
-    DPMeans
-)
+from .dpmeans import MiniBatchDPMeans, DPMeans
```

## pdc_dp_means/dpmeans.py

```diff
@@ -1,1361 +1,1228 @@
-from time import time
-from warnings import warn
-
-import numpy as np
-import scipy.sparse as sp
-
-from sklearn.cluster._k_means_common import CHUNK_SIZE, _inertia_dense
-from sklearn.cluster._k_means_lloyd import lloyd_iter_chunked_dense
-from sklearn.cluster._kmeans import KMeans, _labels_inertia, _labels_inertia_threadpool_limit, _minibatch_update_dense
-from sklearn.exceptions import ConvergenceWarning
-from sklearn.utils import check_array, check_random_state, deprecated
-from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
-from sklearn.utils.extmath import row_norms
-from sklearn.utils.fixes import threadpool_limits
-from sklearn.utils.sparsefuncs_fast import assign_rows_csr
-from sklearn.utils.validation import _check_sample_weight, check_is_fitted
-
-from .dp_means_cython import lloyd_iter_chunked_dense_with_min_sample
-
-
-def _dpmeans_single_lloyd(
-    X,
-    sample_weight,
-    centers_init,
-    max_iter=300,
-    verbose=False,
-    x_squared_norms=None,
-    tol=1e-4,
-    n_threads=1,
-    delta=1.0,
-    max_clusters=None,
-):
-    n_clusters = centers_init.shape[0]
-
-    # Buffers to avoid new allocations at each iteration.
-    centers = centers_init
-    centers_new = np.zeros_like(centers)
-    labels = np.full(X.shape[0], -1, dtype=np.int32)
-    labels_old = labels.copy()
-    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)
-    center_shift = np.zeros(n_clusters, dtype=X.dtype)
-    max_index = np.full(1, -1, dtype=np.int32)
-    max_distance = np.full(1, -1, dtype=centers.dtype)
-
-    lloyd_iter = lloyd_iter_chunked_dense_with_min_sample
-    lloyd_kmeans_iter = lloyd_iter_chunked_dense
-    _inertia = _inertia_dense
-
-    strict_convergence = False
-    iter_time = []
-    # all_centers = [None]
-    small_clusters = 0
-    # Threadpoolctl context to limit the number of threads in second level of
-    # nested parallelism (i.e. BLAS) to avoid oversubsciption.
-    with threadpool_limits(limits=1, user_api="blas"):
-        for i in range(max_iter):
-            tic = time()
-            lloyd_iter(
-                X,
-                sample_weight,
-                x_squared_norms,
-                centers,
-                centers_new,
-                weight_in_clusters,
-                labels,
-                center_shift,
-                max_index,
-                max_distance,
-                n_threads,
-                update_centers=True,
-            )
-
-            if verbose:
-                inertia = _inertia(X, sample_weight, centers, labels, n_threads)
-                print(f"Iteration {i}, inertia {inertia}.")
-            new_cluster = False
-
-            if max_clusters is None or max_clusters > centers.shape[0]:
-                if max_index[0] != -1 and max_distance[0] > delta:
-                    centers = np.vstack((centers, X[max_index])).astype(X.dtype)
-                    centers_new = np.vstack((centers_new, X[max_index])).astype(X.dtype)
-                    weight_in_clusters = np.hstack([weight_in_clusters, [0]]).astype(
-                        X.dtype
-                    )
-                    center_shift = np.hstack([center_shift, [0]]).astype(X.dtype)
-                    new_cluster = True
-
-            # update_centers(X,sample_weight,labels,centers_new,weight_in_clusters)
-
-            centers, centers_new = centers_new, centers
-            toc = time()
-            iter_time.append(toc - tic)
-            # all_centers.append(centers)
-
-            if new_cluster is False:
-                if np.array_equal(labels, labels_old):
-                    # First check the labels for strict convergence.
-                    if verbose:
-                        print(f"Converged at iteration {i}: strict convergence.")
-                    strict_convergence = True
-                    break
-                else:
-                    # No strict convergence, check for tol based convergence.
-                    center_shift_tot = (center_shift ** 2).sum()
-                    if center_shift_tot <= tol:
-                        if verbose:
-                            print(
-                                f"Converged at iteration {i}: center shift "
-                                f"{center_shift_tot} within tolerance {tol}."
-                            )
-                        break
-
-            labels_old[:] = labels
-
-        if not strict_convergence:
-            # rerun E-step so that predicted labels match cluster centers
-            lloyd_kmeans_iter(
-                X,
-                sample_weight,
-                centers,
-                centers,
-                weight_in_clusters,
-                labels,
-                center_shift,
-                n_threads,
-                update_centers=False,
-            )
-
-    inertia = _inertia(X, sample_weight, centers, labels, n_threads)
-
-    return labels, inertia, centers, i + 1, 0, iter_time
-
-
-
-
-class DPMeans(KMeans):
-    """DP-Means clustering.
-
-    The DP-Means is an extension of the K-Means algorithm inspired by the Dirichlet Process Mixture model.
-    This allows the number of clusters to be learned from the data, instead of being set beforehand.
-
-    Parameters
-    ----------
-
-    n_clusters : int, default=8
-        The initial number of clusters to form as well as the number of
-        centroids to generate.
-
-    init : {'k-means++', 'random'}, callable or array-like of shape \
-            (n_clusters, n_features), default='k-means++'
-        Method for initialization. Same as KMeans initialization.
-
-    max_iter : int, default=300
-        Maximum number of iterations of the DP-means algorithm for a
-        single run.
-
-    tol : float, default=1e-4
-        Relative tolerance with regards to Frobenius norm of the difference
-        in the cluster centers of two consecutive iterations to declare
-        convergence.
-
-    verbose : int, default=0
-        Verbosity mode.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines random number generation for centroid initialization.
-
-    copy_x : bool, default=True
-        When pre-computing distances it is more numerically accurate to center
-        the data first.
-
-    delta : float, default=1.0
-        The parameter controls the balance between the number of clusters and the data fitting term.
-        Higher values of delta would generate fewer clusters, lower values would generate more clusters.
-
-    max_clusters : int or None, default=None
-        The maximum number of clusters that can be formed. Useful for controlling runtime
-        in the case where it's suspected that delta is set too low.
-
-    Attributes
-    ----------
-    cluster_centers_ : ndarray of shape (n_clusters, n_features)
-        Coordinates of cluster centers.
-
-    labels_ : ndarray of shape (n_samples,)
-        Labels of each point
-
-    inertia_ : float
-        Sum of squared distances of samples to their closest cluster center,
-        weighted by the sample weights if provided.
-
-    n_iter_ : int
-        Number of iterations run.
-
-    n_features_in_ : int
-        Number of features seen during :term:`fit`.
-
-    feature_names_in_ : ndarray of shape (`n_features_in_`,)
-        Names of features seen during :term:`fit`. Defined only when `X`
-        has feature names that are all strings.
-
-    See Also
-    --------
-    KMeans : The base algorithm for DP-Means. Fixed number of clusters.
-
-    Notes
-    -----
-    The DP-Means algorithm extends K-Means by treating the number of clusters as a variable to be learned.
-    A new cluster is formed whenever a data point is "far enough" from all existing clusters.
-    "Far enough" is determined by the `delta` parameter, which effectively controls the number of clusters formed.
-
-
-    Examples
-    --------
-
-    >>> from pdc_dp_means import DPMeans
-    >>> import numpy as np
-    >>> X = np.array([[1, 2], [1, 4], [1, 0],
-    ...               [10, 2], [10, 4], [10, 0]])
-    >>> dpmeans = DPMeans(n_clusters=2, delta=1.0, random_state=0).fit(X)
-    >>> dpmeans.labels_
-    array([1, 1, 1, 0, 0, 0], dtype=int32)
-    >>> dpmeans.predict([[0, 0], [12, 3]])
-    array([1, 0], dtype=int32)
-    >>> dpmeans.cluster_centers_
-    array([[10.,  2.],
-        [ 1.,  2.]])
-    """
-    def __init__(
-        self,
-        n_clusters=8,
-        *,
-        init="k-means++",
-        n_init=10,
-        max_iter=300,
-        tol=1e-4,
-        verbose=0,
-        random_state=None,
-        copy_x=True,
-        delta=1.0,
-        max_clusters=None,
-    ):
-        
-
-        super().__init__(
-            n_clusters=n_clusters,
-            init=init,
-            max_iter=max_iter,
-            verbose=verbose,
-            random_state=random_state,
-            tol=tol,
-            n_init=n_init,
-            copy_x=copy_x,
-        )
-        self.delta = delta
-        self.max_clusters = max_clusters
-
-    def fit(self, X, y=None, sample_weight=None):
-        """Compute dp-means clustering.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix} of shape (n_samples, n_features)
-            Training instances to cluster. It must be noted that the data
-            will be converted to C ordering, which will cause a memory
-            copy if the given data is not C-contiguous.
-            If a sparse matrix is passed, a copy will be made if it's not in
-            CSR format.
-
-        y : Ignored
-            Not used, present here for API consistency by convention.
-
-        sample_weight : array-like of shape (n_samples,), default=None
-            The weights for each observation in X. If None, all observations
-            are assigned equal weight.
-
-            .. versionadded:: 0.20
-
-        Returns
-        -------
-        self : object
-            Fitted estimator.
-        """
-        X = self._validate_data(
-            X,
-            accept_sparse="csr",
-            dtype=[np.float64, np.float32],
-            order="C",
-            copy=self.copy_x,
-            accept_large_sparse=False,
-        )
-
-        super()._check_params_vs_input(X)
-        random_state = check_random_state(self.random_state)
-        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
-        self._n_threads = _openmp_effective_n_threads()
-
-        # Validate init array
-        init = self.init
-        if hasattr(init, "__array__"):
-            init = check_array(init, dtype=X.dtype, copy=True, order="C")
-            self._validate_center_shape(X, init)
-
-        # subtract of mean of x for more accurate distance computations
-        if not sp.issparse(X):
-            X_mean = X.mean(axis=0)
-            # The copy was already done above
-            X -= X_mean
-
-            if hasattr(init, "__array__"):
-                init -= X_mean
-
-        # precompute squared norms of data points
-        x_squared_norms = row_norms(X, squared=True)
-
-        dpmeans_single = _dpmeans_single_lloyd
-        self._check_mkl_vcomp(X, X.shape[0])
-
-        best_inertia, best_labels = None, None
-
-        for i in range(self._n_init):
-            # Initialize centers
-            centers_init = self._init_centroids(
-                X, x_squared_norms=x_squared_norms, init=init, random_state=random_state, sample_weight=sample_weight
-            )
-
-            if self.verbose:
-                print("Initialization complete")
-
-            # run a dp-means once
-
-            labels, inertia, centers, n_iter_, all_centers, iter_time = dpmeans_single(
-                X,
-                sample_weight,
-                centers_init,
-                max_iter=self.max_iter,
-                verbose=self.verbose,
-                tol=self._tol,
-                x_squared_norms=x_squared_norms,
-                n_threads=self._n_threads,
-                delta=self.delta,
-                max_clusters=self.max_clusters,
-            )
-
-            inertia += self.delta * centers.shape[0]
-
-            # determine if these results are the best so far
-            # we chose a new run if it has a better inertia and the clustering is
-            # different from the best so far (it's possible that the inertia is
-            # slightly better even if the clustering is the same with potentially
-            # permuted labels, due to rounding errors)
-            if best_inertia is None or (inertia < best_inertia):
-                best_labels = labels
-                best_centers = centers
-                best_inertia = inertia
-                best_n_iter = n_iter_
-
-        if not sp.issparse(X):
-            if not self.copy_x:
-                X += X_mean
-            best_centers += X_mean
-        self.n_clusters = best_centers.shape[0]
-        distinct_clusters = len(set(best_labels))
-        if distinct_clusters < self.n_clusters:
-            warnings.warn(
-                "Number of distinct clusters ({}) found smaller than "
-                "n_clusters ({}). Possibly due to duplicate points "
-                "in X.".format(distinct_clusters, self.n_clusters),
-                ConvergenceWarning,
-                stacklevel=2,
-            )
-
-        self.cluster_centers_ = best_centers
-        self.labels_ = best_labels
-        self.inertia_ = best_inertia
-        self.n_iter_ = best_n_iter
-        # self.iter_time = iter_time
-        # self.all_centers = all_centers
-        return self
-
-
-def _mini_batch_step(
-    X,
-    x_squared_norms,
-    sample_weight,
-    centers,
-    centers_new,
-    weight_sums,
-    random_state,
-    random_reassign=False,
-    reassignment_ratio=0.01,
-    verbose=False,
-    n_threads=1,
-):
-    """Incremental update of the centers for the Minibatch K-Means algorithm.
-
-    Parameters
-    ----------
-
-    X : {ndarray, sparse matrix} of shape (n_samples, n_features)
-        The original data array. If sparse, must be in CSR format.
-
-    x_squared_norms : ndarray of shape (n_samples,)
-        Squared euclidean norm of each data point.
-
-    sample_weight : ndarray of shape (n_samples,)
-        The weights for each observation in X.
-
-    centers : ndarray of shape (n_clusters, n_features)
-        The cluster centers before the current iteration
-
-    centers_new : ndarray of shape (n_clusters, n_features)
-        The cluster centers after the current iteration. Modified in-place.
-
-    weight_sums : ndarray of shape (n_clusters,)
-        The vector in which we keep track of the numbers of points in a
-        cluster. This array is modified in place.
-
-    random_state : RandomState instance
-        Determines random number generation for low count centers reassignment.
-        See :term:`Glossary <random_state>`.
-
-    random_reassign : boolean, default=False
-        If True, centers with very low counts are randomly reassigned
-        to observations.
-
-    reassignment_ratio : float, default=0.01
-        Control the fraction of the maximum number of counts for a
-        center to be reassigned. A higher value means that low count
-        centers are more likely to be reassigned, which means that the
-        model will take longer to converge, but should converge in a
-        better clustering.
-
-    verbose : bool, default=False
-        Controls the verbosity.
-
-    n_threads : int, default=1
-        The number of OpenMP threads to use for the computation.
-
-    Returns
-    -------
-    inertia : float
-        Sum of squared distances of samples to their closest cluster center.
-        The inertia is computed after finding the labels and before updating
-        the centers.
-    """
-    # Perform label assignment to nearest centers
-    # For better efficiency, it's better to run _mini_batch_step in a
-    # threadpool_limit context than using _labels_inertia_threadpool_limit here
-    labels, inertia = _labels_inertia(
-        X, sample_weight, x_squared_norms, centers, n_threads=n_threads
-    )
-
-    # Update centers according to the labels
-    if sp.issparse(X):
-        _minibatch_update_sparse(
-            X, sample_weight, centers, centers_new, weight_sums, labels, n_threads
-        )
-    else:
-        _minibatch_update_dense(
-            X,
-            sample_weight,
-            centers,
-            centers_new,
-            weight_sums,
-            labels,
-            n_threads,
-        )
-
-    # Reassign clusters that have very low weight
-    if random_reassign and reassignment_ratio > 0:
-        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()
-
-        # pick at most .5 * batch_size samples as new centers
-        if to_reassign.sum() > 0.5 * X.shape[0]:
-            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]
-            to_reassign[indices_dont_reassign] = False
-        n_reassigns = to_reassign.sum()
-
-        if n_reassigns:
-            # Pick new clusters amongst observations with uniform probability
-            new_centers = random_state.choice(
-                X.shape[0], replace=False, size=n_reassigns
-            )
-            if verbose:
-                print(f"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.")
-
-            if sp.issparse(X):
-                assign_rows_csr(
-                    X,
-                    new_centers.astype(np.intp, copy=False),
-                    np.where(to_reassign)[0].astype(np.intp, copy=False),
-                    centers_new,
-                )
-            else:
-                centers_new[to_reassign] = X[new_centers]
-
-        # reset counts of reassigned centers, but don't reset them too small
-        # to avoid instant reassignment. This is a pretty dirty hack as it
-        # also modifies the learning rates.
-        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])
-
-    return inertia
-
-
-def _labels_inertia_with_min_sample(
-    X, sample_weight, x_squared_norms, centers, n_threads=1
-):
-    """E step of the K-means EM algorithm.
-
-    Compute the labels and the inertia of the given samples and centers.
-
-    Parameters
-    ----------
-    X : {ndarray, sparse matrix} of shape (n_samples, n_features)
-        The input samples to assign to the labels. If sparse matrix, must
-        be in CSR format.
-
-    sample_weight : ndarray of shape (n_samples,)
-        The weights for each observation in X.
-
-    x_squared_norms : ndarray of shape (n_samples,)
-        Precomputed squared euclidean norm of each data point, to speed up
-        computations.
-
-    centers : ndarray of shape (n_clusters, n_features)
-        The cluster centers.
-
-    n_threads : int, default=1
-        The number of OpenMP threads to use for the computation. Parallelism is
-        sample-wise on the main cython loop which assigns each sample to its
-        closest center.
-
-    Returns
-    -------
-    labels : ndarray of shape (n_samples,)
-        The resulting assignment.
-
-    inertia : float
-        Sum of squared distances of samples to their closest cluster center.
-    """
-    n_samples = X.shape[0]
-    n_clusters = centers.shape[0]
-
-    labels = np.full(n_samples, -1, dtype=np.int32)
-    max_index = np.full(1, -1, dtype=np.int32)
-    max_distance = np.full(1, -1, dtype=centers.dtype)
-    weight_in_clusters = np.zeros(n_clusters, dtype=centers.dtype)
-    center_shift = np.zeros_like(weight_in_clusters)
-
-
-    _labels = lloyd_iter_chunked_dense_with_min_sample
-    _inertia = _inertia_dense
-    X = X
-
-    _labels(
-        X,
-        sample_weight,
-        x_squared_norms,
-        centers,
-        centers,
-        weight_in_clusters,
-        labels,
-        center_shift,
-        max_index,
-        max_distance,
-        n_threads,
-        update_centers=False,
-    )
-
-    inertia = _inertia(X, sample_weight, centers, labels, n_threads)
-
-    return labels, inertia, max_index, max_distance
-
-
-def _mini_batch_step_with_max_distance(
-    X,
-    x_squared_norms,
-    sample_weight,
-    centers,
-    centers_new,
-    weight_sums,
-    random_state,
-    random_reassign=False,
-    reassignment_ratio=0.01,
-    verbose=False,
-    n_threads=1,
-):
-    """Incremental update of the centers for the Minibatch K-Means algorithm.
-
-    Parameters
-    ----------
-
-    X : {ndarray, sparse matrix} of shape (n_samples, n_features)
-        The original data array. If sparse, must be in CSR format.
-
-    x_squared_norms : ndarray of shape (n_samples,)
-        Squared euclidean norm of each data point.
-
-    sample_weight : ndarray of shape (n_samples,)
-        The weights for each observation in X.
-
-    centers : ndarray of shape (n_clusters, n_features)
-        The cluster centers before the current iteration
-
-    centers_new : ndarray of shape (n_clusters, n_features)
-        The cluster centers after the current iteration. Modified in-place.
-
-    weight_sums : ndarray of shape (n_clusters,)
-        The vector in which we keep track of the numbers of points in a
-        cluster. This array is modified in place.
-
-    random_state : RandomState instance
-        Determines random number generation for low count centers reassignment.
-        See :term:`Glossary <random_state>`.
-
-    random_reassign : boolean, default=False
-        If True, centers with very low counts are randomly reassigned
-        to observations.
-
-    reassignment_ratio : float, default=0.01
-        Control the fraction of the maximum number of counts for a
-        center to be reassigned. A higher value means that low count
-        centers are more likely to be reassigned, which means that the
-        model will take longer to converge, but should converge in a
-        better clustering.
-
-    verbose : bool, default=False
-        Controls the verbosity.
-
-    n_threads : int, default=1
-        The number of OpenMP threads to use for the computation.
-
-    Returns
-    -------
-    inertia : float
-        Sum of squared distances of samples to their closest cluster center.
-        The inertia is computed after finding the labels and before updating
-        the centers.
-    """
-    # Perform label assignment to nearest centers
-    # For better efficiency, it's better to run _mini_batch_step in a
-    # threadpool_limit context than using _labels_inertia_threadpool_limit here
-    labels, inertia, max_index, max_distance = _labels_inertia_with_min_sample(
-        X, sample_weight, x_squared_norms, centers, n_threads=n_threads
-    )
-
-    # Update centers according to the labels
-
-    _minibatch_update_dense(
-        X,
-        sample_weight,
-        centers,
-        centers_new,
-        weight_sums,
-        labels,
-        n_threads,
-    )
-
-    # Reassign clusters that have very low weight
-
-    if random_reassign and reassignment_ratio > 0:
-        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()
-
-        # pick at most .5 * batch_size samples as new centers
-        if to_reassign.sum() > 0.5 * X.shape[0]:
-            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]
-            to_reassign[indices_dont_reassign] = False
-        n_reassigns = to_reassign.sum()
-
-        if n_reassigns:
-            # Pick new clusters amongst observations with uniform probability
-            new_centers = random_state.choice(
-                X.shape[0], replace=False, size=n_reassigns
-            )
-            if verbose:
-                print(f"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.")
-
-            if sp.issparse(X):
-                assign_rows_csr(
-                    X,
-                    new_centers.astype(np.intp, copy=False),
-                    np.where(to_reassign)[0].astype(np.intp, copy=False),
-                    centers_new,
-                )
-            else:
-                centers_new[to_reassign] = X[new_centers]
-
-        # reset counts of reassigned centers, but don't reset them too small
-        # to avoid instant reassignment. This is a pretty dirty hack as it
-        # also modifies the learning rates.
-        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])
-
-    return inertia, max_index[0], max_distance[0]
-
-
-class MiniBatchDPMeans(KMeans):
-    '''
-    Parameters
-    ----------
-
-    n_clusters : int, default=1
-        The initial number of clusters to form as well as the number of
-        centroids to generate.
-
-    init : {'k-means++', 'random'}, callable or array-like of shape \
-            (n_clusters, n_features), default='k-means++'
-        Method for initialization:
-
-        'k-means++' : selects initial cluster centroids for deterministic 
-        initialization using sampling based on an empirical probability distribution 
-        of the points' contribution to the overall inertia. This technique 
-        speeds up convergence. The algorithm implemented is "greedy k-means++". 
-        It differs from the vanilla k-means++ by making several trials at each 
-        sampling step and choosing the best centroid among them.
-
-        'random': choose `n_clusters` observations (rows) at random from data
-        for the initial centroids.
-
-        If an array is passed, it should be of shape (n_clusters, n_features)
-        and gives the initial centers.
-
-        If a callable is passed, it should take arguments X, n_clusters and a
-        random state and return an initialization.
-
-    max_iter : int, default=100
-        Maximum number of iterations over the complete dataset before
-        stopping independently of any early stopping criterion heuristics.
-
-    batch_size : int, default=1024
-        Size of the mini batches.
-        For faster computations, you can set the ``batch_size`` greater than
-        256 * number of cores to enable parallelism on all cores.
-
-    verbose : int, default=0
-        Verbosity mode.
-
-    compute_labels : bool, default=True
-        Compute label assignment and inertia for the complete dataset
-        once the minibatch optimization has converged in fit.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines random number generation for centroid initialization and
-        random reassignment. Use an int to make the randomness deterministic.
-        See :term:`Glossary <random_state>`.
-
-    tol : float, default=0.0
-        Control early stopping based on the relative center changes as
-        measured by a smoothed, variance-normalized of the mean center
-        squared position changes.
-
-    max_no_improvement : int, default=10
-        Control early stopping based on the consecutive number of mini
-        batches that does not yield an improvement on the smoothed inertia.
-
-    init_size : int, default=None
-        Number of samples to randomly sample for speeding up the
-        initialization (sometimes at the expense of accuracy): the
-        only algorithm is initialized by running a batch DPMeans on a
-        random subset of the data. This needs to be larger than n_clusters.
-
-    n_init : int, default=3
-        Number of random initializations that are tried.
-        The algorithm is only run once, using the best of
-        the `n_init` initializations as measured by inertia.
-
-    reassignment_ratio : float, default=0.01
-        Control the fraction of the maximum number of counts for a center to
-        be reassigned. A higher value means that low count centers are more
-        easily reassigned, which means that the model will take longer to
-        converge, but should converge in a better clustering.
-
-    delta : float, default=1.0
-        Parameter controlling the number of clusters in the DP-means
-        algorithm. A higher value will lead to fewer clusters.
-
-    Attributes
-    ----------
-        cluster_centers_ : ndarray of shape (n_clusters, n_features)
-            Coordinates of cluster centers.
-
-        labels_ : ndarray of shape (n_samples,)
-            Labels of each point (if compute_labels is set to True).
-
-        inertia_ : float
-            The value of the inertia criterion associated with the chosen
-            partition if compute_labels is set to True. If compute_labels is set to
-            False, it's an approximation of the inertia based on an exponentially
-            weighted average of the batch inertiae.
-            The inertia is defined as the sum of square distances of samples to
-            their cluster center, weighted by the sample weights if provided.
-
-        n_iter_ : int
-            Number of iterations over the full dataset.
-
-        n_features_in_ : int
-            Number of features seen during :term:`fit`.
-
-        feature_names_in_ : ndarray of shape (`n_features_in_`,)
-            Names of features seen during :term:`fit`. Defined only when `X`
-            has feature names that are all strings.
-
-        See Also
-        --------
-        DPMeans : The full batch version of DP-Means clustering.
-
-        KMeans : The classic implementation of the clustering method based on the
-            Lloyd's algorithm. It consumes the whole set of input data at each
-            iteration.
-
-        Notes
-        -----
-        When there are too few points in the dataset, some centers may be
-        duplicated, which means that a proper clustering in terms of the number
-        of requesting clusters and the number of returned clusters will not
-        always match. One solution is to set `reassignment_ratio=0`, which
-        prevents reassignments of clusters that are too small.
-
-        Examples
-        --------
-        >>> from pdc_dp_means import MiniBatchDPMeans
-        >>> import numpy as np
-        >>> X = np.array([[1, 2], [1, 4], [1, 0],
-        ...               [4, 2], [4, 0], [4, 4],
-        ...               [4, 5], [0, 1], [2, 2],
-        ...               [3, 2], [5, 5], [1, -1]])
-        >>> # manually fit on batches
-        >>> dpmeans = MiniBatchDPMeans(n_clusters=1,
-        ...                            random_state=0,
-        ...                            batch_size=6,
-        ...                            n_init=3,
-        ...                            delta=1.0)
-        >>> dpmeans = dpmeans.partial_fit(X[0:6,:])
-        >>> dpmeans = dpmeans.partial_fit(X[6:12,:])
-        >>> dpmeans.cluster_centers_
-        array([[3.375, 3.  ],
-            [0.75 , 0.5 ]])
-        >>> dpmeans.predict([[0, 0], [4, 4]])
-        array([1, 0], dtype=int32)
-        >>> # fit on the whole data
-        >>> dpmeans = MiniBatchDPMeans(n_clusters=1,
-        ...                            random_state=0,
-        ...                            batch_size=6,
-        ...                            max_iter=10,
-        ...                            n_init=3,
-        ...                            delta=1.0).fit(X)
-        >>> dpmeans.cluster_centers_
-        array([[3.55102041, 2.48979592],
-            [1.06896552, 1.        ]])
-        >>> dpmeans.predict([[0, 0], [4, 4]])
-        array([1, 0], dtype=int32)
-
-    
-    '''
-    def __init__(
-        self,
-        n_clusters=1,
-        *,
-        init="k-means++",
-        max_iter=100,
-        batch_size=1024,
-        verbose=0,
-        compute_labels=True,
-        random_state=None,
-        tol=0.0,
-        max_no_improvement=10,
-        init_size=None,
-        n_init=3,
-        reassignment_ratio=0.01,
-        delta=1.0,
-    ):
-
-        super().__init__(
-            n_clusters=n_clusters,
-            init=init,
-            max_iter=max_iter,
-            verbose=verbose,
-            random_state=random_state,
-            tol=tol,
-            n_init=n_init,
-        )
-
-        self.max_no_improvement = max_no_improvement
-        self.batch_size = batch_size
-        self.compute_labels = compute_labels
-        self.init_size = init_size
-        self.reassignment_ratio = reassignment_ratio
-        self.delta = delta
-
-    @deprecated(  # type: ignore
-        "The attribute `counts_` is deprecated in 0.24"
-        " and will be removed in 1.1 (renaming of 0.26)."
-    )
-    @property
-    def counts_(self):
-        return self._counts
-
-    @deprecated(  # type: ignore
-        "The attribute `init_size_` is deprecated in "
-        "0.24 and will be removed in 1.1 (renaming of 0.26)."
-    )
-    @property
-    def init_size_(self):
-        return self._init_size
-
-    @deprecated(  # type: ignore
-        "The attribute `random_state_` is deprecated "
-        "in 0.24 and will be removed in 1.1 (renaming of 0.26)."
-    )
-    @property
-    def random_state_(self):
-        return getattr(self, "_random_state", None)
-
-    def _check_params(self, X):
-        super()._check_params_vs_input(X)
-
-        # max_no_improvement
-        if self.max_no_improvement is not None and self.max_no_improvement < 0:
-            raise ValueError(
-                "max_no_improvement should be >= 0, got "
-                f"{self.max_no_improvement} instead."
-            )
-
-        # batch_size
-        if self.batch_size <= 0:
-            raise ValueError(
-                f"batch_size should be > 0, got {self.batch_size} instead."
-            )
-        self._batch_size = min(self.batch_size, X.shape[0])
-
-        # init_size
-        if self.init_size is not None and self.init_size <= 0:
-            raise ValueError(f"init_size should be > 0, got {self.init_size} instead.")
-        self._init_size = self.init_size
-        if self._init_size is None:
-            self._init_size = 3 * self._batch_size
-            if self._init_size < self.n_clusters:
-                self._init_size = 3 * self.n_clusters
-        elif self._init_size < self.n_clusters:
-            warnings.warn(
-                f"init_size={self._init_size} should be larger than "
-                f"n_clusters={self.n_clusters}. Setting it to "
-                "min(3*n_clusters, n_samples)",
-                RuntimeWarning,
-                stacklevel=2,
-            )
-            self._init_size = 3 * self.n_clusters
-        self._init_size = min(self._init_size, X.shape[0])
-
-        # reassignment_ratio
-        if self.reassignment_ratio < 0:
-            raise ValueError(
-                "reassignment_ratio should be >= 0, got "
-                f"{self.reassignment_ratio} instead."
-            )
-
-    def _mini_batch_convergence(
-        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia
-    ):
-        """Helper function to encapsulate the early stopping logic"""
-        # Normalize inertia to be able to compare values when
-        # batch_size changes
-        batch_inertia /= self._batch_size
-
-        # count steps starting from 1 for user friendly verbose mode.
-        step = step + 1
-
-        # Ignore first iteration because it's inertia from initialization.
-        if step == 1:
-            if self.verbose:
-                print(
-                    f"Minibatch step {step}/{n_steps}: mean batch "
-                    f"inertia: {batch_inertia}"
-                )
-            return False
-
-        # Compute an Exponentially Weighted Average of the inertia to
-        # monitor the convergence while discarding minibatch-local stochastic
-        # variability: https://en.wikipedia.org/wiki/Moving_average
-        if self._ewa_inertia is None:
-            self._ewa_inertia = batch_inertia
-        else:
-            alpha = self._batch_size * 2.0 / (n_samples + 1)
-            alpha = min(alpha, 1)
-            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha
-
-        # Log progress to be able to monitor convergence
-        if self.verbose:
-            print(
-                f"Minibatch step {step}/{n_steps}: mean batch inertia: "
-                f"{batch_inertia}, ewa inertia: {self._ewa_inertia}"
-            )
-
-        # Early stopping based on absolute tolerance on squared change of
-        # centers position
-        if self._tol > 0.0 and centers_squared_diff <= self._tol:
-            if self.verbose:
-                print(f"Converged (small centers change) at step {step}/{n_steps}")
-            return True
-
-        # Early stopping heuristic due to lack of improvement on smoothed
-        # inertia
-        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:
-            self._no_improvement = 0
-            self._ewa_inertia_min = self._ewa_inertia
-        else:
-            self._no_improvement += 1
-
-        if (
-            self.max_no_improvement is not None
-            and self._no_improvement >= self.max_no_improvement
-        ):
-            if self.verbose:
-                print(
-                    "Converged (lack of improvement in inertia) at step "
-                    f"{step}/{n_steps}"
-                )
-            return True
-
-        return False
-
-    def _random_reassign(self):
-        """Check if a random reassignment needs to be done.
-
-        Do random reassignments each time 10 * n_clusters samples have been
-        processed.
-
-        If there are empty clusters we always want to reassign.
-        """
-        self._n_since_last_reassign += self._batch_size
-        if (self._counts == 0).any() or self._n_since_last_reassign >= (
-            10 * self.n_clusters
-        ):
-            self._n_since_last_reassign = 0
-            return True
-        return False
-
-    def fit(self, X, y=None, sample_weight=None):
-        """Compute the centroids on X by chunking it into mini-batches.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix} of shape (n_samples, n_features)
-            Training instances to cluster. It must be noted that the data
-            will be converted to C ordering, which will cause a memory copy
-            if the given data is not C-contiguous.
-            If a sparse matrix is passed, a copy will be made if it's not in
-            CSR format.
-
-        y : Ignored
-            Not used, present here for API consistency by convention.
-
-        sample_weight : array-like of shape (n_samples,), default=None
-            The weights for each observation in X. If None, all observations
-            are assigned equal weight.
-
-            .. versionadded:: 0.20
-
-        Returns
-        -------
-        self : object
-            Fitted estimator.
-        """
-        X = self._validate_data(
-            X,
-            accept_sparse="csr",
-            dtype=[np.float64, np.float32],
-            order="C",
-            accept_large_sparse=False,
-        )
-
-        self._check_params(X)
-        random_state = check_random_state(self.random_state)
-        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
-        self._n_threads = _openmp_effective_n_threads()
-        n_samples, n_features = X.shape
-
-        # Validate init array
-        init = self.init
-        if hasattr(init, "__array__"):
-            init = check_array(init, dtype=X.dtype, copy=True, order="C")
-            self._validate_center_shape(X, init)
-
-        self._check_mkl_vcomp(X, self._batch_size)
-
-        # precompute squared norms of data points
-        x_squared_norms = row_norms(X, squared=True)
-
-        # Validation set for the init
-        validation_indices = random_state.randint(0, n_samples, self._init_size)
-        X_valid = X[validation_indices]
-        sample_weight_valid = sample_weight[validation_indices]
-
-        # perform several inits with random subsets
-        best_inertia = None
-        for init_idx in range(self._n_init):
-            if self.verbose:
-                print(f"Init {init_idx + 1}/{self._n_init} with method {init}")
-
-            # Initialize the centers using only a fraction of the data as we
-            # expect n_samples to be very large when using MiniBatchKMeans.
-            cluster_centers = self._init_centroids(
-                X,
-                x_squared_norms=x_squared_norms,
-                init=init,
-                random_state=random_state,
-                init_size=self._init_size,
-                sample_weight=sample_weight
-            )
-
-            # Compute inertia on a validation set.
-            _, inertia = _labels_inertia_threadpool_limit(
-                X_valid,
-                sample_weight_valid,
-                cluster_centers,
-                n_threads=self._n_threads,
-            )
-
-            if self.verbose:
-                print(f"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}")
-            if best_inertia is None or inertia < best_inertia:
-                init_centers = cluster_centers
-                best_inertia = inertia
-
-        centers = init_centers
-        centers_new = np.empty_like(centers)
-
-        # Initialize counts
-        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)
-
-        # Attributes to monitor the convergence
-        self._ewa_inertia = None
-        self._ewa_inertia_min = None
-        self._no_improvement = 0
-
-        # Initialize number of samples seen since last reassignment
-        self._n_since_last_reassign = 0
-
-        n_steps = (self.max_iter * n_samples) // self._batch_size
-
-        iter_time = [0]
-        self.iter_centers = [None]
-
-        with threadpool_limits(limits=1, user_api="blas"):
-            # Perform the iterative optimization until convergence
-            for i in range(n_steps):
-                # Sample a minibatch from the full dataset
-                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)
-                tic = time()
-                # Perform the actual update step on the minibatch data
-                (
-                    batch_inertia,
-                    max_index,
-                    max_distance,
-                ) = _mini_batch_step_with_max_distance(
-                    X=X[minibatch_indices],
-                    x_squared_norms=x_squared_norms[minibatch_indices],
-                    sample_weight=sample_weight[minibatch_indices],
-                    centers=centers,
-                    centers_new=centers_new,
-                    weight_sums=self._counts,
-                    random_state=random_state,
-                    random_reassign=self._random_reassign(),
-                    reassignment_ratio=self.reassignment_ratio,
-                    verbose=self.verbose,
-                    n_threads=self._n_threads,
-                )
-                toc = time() - tic
-                iter_time.append(toc)
-                new_cluster = False
-                if max_index != -1 and max_distance > self.delta:
-                    centers = np.vstack((centers, X[minibatch_indices[max_index]])).astype(X.dtype)
-                    centers_new = np.vstack((centers_new, X[minibatch_indices[max_index]])).astype(X.dtype)
-                    self.n_clusters += 1
-                    self._counts = np.hstack([self._counts, [1]]).astype(X.dtype)
-                    new_cluster = True
-
-                if self._tol > 0.0:
-                    centers_squared_diff = np.sum((centers_new - centers) ** 2)
-                else:
-                    centers_squared_diff = 0
-                centers, centers_new = centers_new, centers
-                self.iter_centers.append(centers)
-                # Monitor convergence and do early stopping if necessary
-                if new_cluster is False and self._mini_batch_convergence(
-                    i, n_steps, n_samples, centers_squared_diff, batch_inertia
-                ):
-                    break
-
-        self.cluster_centers_ = centers
-        self.iter_time = iter_time
-        self.n_steps_ = i + 1
-        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))
-
-        if self.compute_labels:
-            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(
-                X,
-                sample_weight,
-                self.cluster_centers_,
-                n_threads=self._n_threads,
-            )
-            self.inertia_ += self.delta * self.cluster_centers_.shape[0]
-        else:
-            self.inertia_ = (
-                self._ewa_inertia * n_samples
-                + self.delta * self.cluster_centers_.shape[0]
-            )
-
-        return self
-
-    def partial_fit(self, X, y=None, sample_weight=None):
-        """Update k means estimate on a single mini-batch X.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix} of shape (n_samples, n_features)
-            Training instances to cluster. It must be noted that the data
-            will be converted to C ordering, which will cause a memory copy
-            if the given data is not C-contiguous.
-            If a sparse matrix is passed, a copy will be made if it's not in
-            CSR format.
-
-        y : Ignored
-            Not used, present here for API consistency by convention.
-
-        sample_weight : array-like of shape (n_samples,), default=None
-            The weights for each observation in X. If None, all observations
-            are assigned equal weight.
-
-        Returns
-        -------
-        self : object
-            Return updated estimator.
-        """
-        has_centers = hasattr(self, "cluster_centers_")
-
-        X = self._validate_data(
-            X,
-            accept_sparse="csr",
-            dtype=[np.float64, np.float32],
-            order="C",
-            accept_large_sparse=False,
-            reset=not has_centers,
-        )
-
-        self._random_state = getattr(
-            self, "_random_state", check_random_state(self.random_state)
-        )
-        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
-        self.n_steps_ = getattr(self, "n_steps_", 0)
-
-        # precompute squared norms of data points
-        x_squared_norms = row_norms(X, squared=True)
-
-        if not has_centers:
-            # this instance has not been fitted yet (fit or partial_fit)
-            self._check_params(X)
-            self._n_threads = _openmp_effective_n_threads()
-
-            # Validate init array
-            init = self.init
-            if hasattr(init, "__array__"):
-                init = check_array(init, dtype=X.dtype, copy=True, order="C")
-                self._validate_center_shape(X, init)
-
-            self._check_mkl_vcomp(X, X.shape[0])
-
-            # initialize the cluster centers
-            self.cluster_centers_ = self._init_centroids(
-                X,
-                x_squared_norms=x_squared_norms,
-                init=init,
-                random_state=self._random_state,
-                sample_weight=sample_weight,
-                init_size=self._init_size,
-            )
-
-            # Initialize counts
-            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)
-
-            # Initialize number of samples seen since last reassignment
-            self._n_since_last_reassign = 0
-
-        with threadpool_limits(limits=1, user_api="blas"):
-            _, max_index, max_distance = _mini_batch_step_with_max_distance(
-                X,
-                x_squared_norms=x_squared_norms,
-                sample_weight=sample_weight,
-                centers=self.cluster_centers_,
-                centers_new=self.cluster_centers_,
-                weight_sums=self._counts,
-                random_state=self._random_state,
-                random_reassign=self._random_reassign(),
-                reassignment_ratio=self.reassignment_ratio,
-                verbose=self.verbose,
-                n_threads=self._n_threads,
-            )
-
-            if max_index != -1 and max_distance > self.delta:
-                self.cluster_centers_ = np.vstack(
-                    (self.cluster_centers_, X[max_index])
-                ).astype(X.dtype)
-                self.n_clusters += 1
-                self._counts = np.hstack([self._counts, [1]]).astype(X.dtype)
-
-        if self.compute_labels:
-            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(
-                X,
-                sample_weight,
-                self.cluster_centers_,
-                n_threads=self._n_threads,
-            )
-            self.inertia_ += self.delta * self.cluster_centers_.shape[0]
-
-        self.n_steps_ += 1
-
-        return self
-
-    def predict(self, X, sample_weight=None):
-        """Predict the closest cluster each sample in X belongs to.
-
-        In the vector quantization literature, `cluster_centers_` is called
-        the code book and each value returned by `predict` is the index of
-        the closest code in the code book.
-
-        Parameters
-        ----------
-        X : {array-like, sparse matrix} of shape (n_samples, n_features)
-            New data to predict.
-
-        sample_weight : array-like of shape (n_samples,), default=None
-            The weights for each observation in X. If None, all observations
-            are assigned equal weight.
-
-        Returns
-        -------
-        labels : ndarray of shape (n_samples,)
-            Index of the cluster each sample belongs to.
-        """
-        check_is_fitted(self)
-
-        X = self._check_test_data(X)
-        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
-
-        labels, _ = _labels_inertia_threadpool_limit(
-            X,
-            sample_weight,
-            self.cluster_centers_,
-            n_threads=self._n_threads,
-        )
-
-        return labels
-
-    def _more_tags(self):
-        return {
-            "_xfail_checks": {
-                "check_sample_weights_invariance": (
-                    "zero sample_weight is not equivalent to removing samples"
-                ),
-            }
-        }
+from time import time
+
+
+import numpy as np
+import scipy.sparse as sp
+
+from sklearn.cluster._k_means_common import _inertia_dense
+from sklearn.cluster._k_means_lloyd import lloyd_iter_chunked_dense
+from sklearn.cluster._kmeans import (
+    KMeans,
+    _labels_inertia_threadpool_limit,
+    _minibatch_update_dense,
+)
+
+from sklearn.utils import check_array, check_random_state, deprecated
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
+from sklearn.utils.extmath import row_norms
+from sklearn.utils.fixes import threadpool_limits
+from sklearn.utils.sparsefuncs_fast import assign_rows_csr
+from sklearn.utils.validation import _check_sample_weight, check_is_fitted
+
+from .dp_means_cython import lloyd_iter_chunked_dense_with_min_sample
+
+
+def _dpmeans_single_lloyd(
+    X,
+    sample_weight,
+    centers_init,
+    max_iter=300,
+    verbose=False,
+    x_squared_norms=None,
+    tol=1e-4,
+    n_threads=1,
+    delta=1.0,
+    max_clusters=None,
+):
+    n_clusters = centers_init.shape[0]
+
+    # Buffers to avoid new allocations at each iteration.
+    centers = centers_init
+    centers_new = np.zeros_like(centers)
+    labels = np.full(X.shape[0], -1, dtype=np.int32)
+    labels_old = labels.copy()
+    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)
+    center_shift = np.zeros(n_clusters, dtype=X.dtype)
+    max_index = np.full(1, -1, dtype=np.int32)
+    max_distance = np.full(1, -1, dtype=centers.dtype)
+
+    lloyd_iter = lloyd_iter_chunked_dense_with_min_sample
+    lloyd_kmeans_iter = lloyd_iter_chunked_dense
+    _inertia = _inertia_dense
+
+    strict_convergence = False
+    iter_time = []
+    # all_centers = [None]
+    # Threadpoolctl context to limit the number of threads in second level of
+    # nested parallelism (i.e. BLAS) to avoid oversubsciption.
+    with threadpool_limits(limits=1, user_api="blas"):
+        for i in range(max_iter):
+            tic = time()
+            lloyd_iter(
+                X,
+                sample_weight,
+                x_squared_norms,
+                centers,
+                centers_new,
+                weight_in_clusters,
+                labels,
+                center_shift,
+                max_index,
+                max_distance,
+                n_threads,
+                update_centers=True,
+            )
+
+            if verbose:
+                inertia = _inertia(X, sample_weight, centers, labels, n_threads)
+                print(f"Iteration {i}, inertia {inertia}.")
+            new_cluster = False
+
+            if max_clusters is None or max_clusters > centers.shape[0]:
+                if max_index[0] != -1 and max_distance[0] > delta:
+                    centers = np.vstack((centers, X[max_index])).astype(X.dtype)
+                    centers_new = np.vstack((centers_new, X[max_index])).astype(
+                        X.dtype
+                    )
+                    weight_in_clusters = np.hstack([weight_in_clusters, [0]]).astype(
+                        X.dtype
+                    )
+                    center_shift = np.hstack([center_shift, [0]]).astype(X.dtype)
+                    new_cluster = True
+
+            # update_centers(X,sample_weight,labels,centers_new,weight_in_clusters)
+
+            centers, centers_new = centers_new, centers
+            toc = time()
+            iter_time.append(toc - tic)
+            # all_centers.append(centers)
+
+            if new_cluster is False:
+                if np.array_equal(labels, labels_old):
+                    # First check the labels for strict convergence.
+                    if verbose:
+                        print(f"Converged at iteration {i}: strict convergence.")
+                    strict_convergence = True
+                    break
+                else:
+                    # No strict convergence, check for tol based convergence.
+                    center_shift_tot = (center_shift**2).sum()
+                    if center_shift_tot <= tol:
+                        if verbose:
+                            print(
+                                f"Converged at iteration {i}: center shift "
+                                f"{center_shift_tot} within tolerance {tol}."
+                            )
+                        break
+
+            labels_old[:] = labels
+
+        if not strict_convergence:
+            # rerun E-step so that predicted labels match cluster centers
+            lloyd_kmeans_iter(
+                X,
+                sample_weight,
+                centers,
+                centers,
+                weight_in_clusters,
+                labels,
+                center_shift,
+                n_threads,
+                update_centers=False,
+            )
+
+    inertia = _inertia(X, sample_weight, centers, labels, n_threads)
+
+    return labels, inertia, centers, i + 1, 0, iter_time
+
+
+class DPMeans(KMeans):
+    """DP-Means clustering.
+
+    The DP-Means is an extension of the K-Means algorithm inspired by the Dirichlet Process Mixture model.
+    This allows the number of clusters to be learned from the data, instead of being set beforehand.
+
+    Parameters
+    ----------
+
+    n_clusters : int, default=8
+        The initial number of clusters to form as well as the number of
+        centroids to generate.
+
+    init : {'k-means++', 'random'}, callable or array-like of shape \
+            (n_clusters, n_features), default='k-means++'
+        Method for initialization. Same as KMeans initialization.
+
+    max_iter : int, default=300
+        Maximum number of iterations of the DP-means algorithm for a
+        single run.
+
+    tol : float, default=1e-4
+        Relative tolerance with regards to Frobenius norm of the difference
+        in the cluster centers of two consecutive iterations to declare
+        convergence.
+
+    verbose : int, default=0
+        Verbosity mode.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines random number generation for centroid initialization.
+
+    copy_x : bool, default=True
+        When pre-computing distances it is more numerically accurate to center
+        the data first.
+
+    delta : float, default=1.0
+        The parameter controls the balance between the number of clusters and the data fitting term.
+        Higher values of delta would generate fewer clusters, lower values would generate more clusters.
+
+    max_clusters : int or None, default=None
+        The maximum number of clusters that can be formed. Useful for controlling runtime
+        in the case where it's suspected that delta is set too low.
+
+    Attributes
+    ----------
+    cluster_centers_ : ndarray of shape (n_clusters, n_features)
+        Coordinates of cluster centers.
+
+    labels_ : ndarray of shape (n_samples,)
+        Labels of each point
+
+    inertia_ : float
+        Sum of squared distances of samples to their closest cluster center,
+        weighted by the sample weights if provided.
+
+    n_iter_ : int
+        Number of iterations run.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+    See Also
+    --------
+    KMeans : The base algorithm for DP-Means. Fixed number of clusters.
+
+    Notes
+    -----
+    The DP-Means algorithm extends K-Means by treating the number of clusters as a variable to be learned.
+    A new cluster is formed whenever a data point is "far enough" from all existing clusters.
+    "Far enough" is determined by the `delta` parameter, which effectively controls the number of clusters formed.
+
+
+    Examples
+    --------
+
+    >>> from pdc_dp_means import DPMeans
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [10, 2], [10, 4], [10, 0]])
+    >>> dpmeans = DPMeans(n_clusters=2, delta=1.0, random_state=0).fit(X)
+    >>> dpmeans.labels_
+    array([1, 1, 1, 0, 0, 0], dtype=int32)
+    >>> dpmeans.predict([[0, 0], [12, 3]])
+    array([1, 0], dtype=int32)
+    >>> dpmeans.cluster_centers_
+    array([[10.,  2.],
+        [ 1.,  2.]])
+    """
+
+    def __init__(
+        self,
+        n_clusters=8,
+        *,
+        init="k-means++",
+        n_init=10,
+        max_iter=300,
+        tol=1e-4,
+        verbose=0,
+        random_state=None,
+        copy_x=True,
+        delta=1.0,
+        max_clusters=None,
+    ):
+        super().__init__(
+            n_clusters=n_clusters,
+            init=init,
+            max_iter=max_iter,
+            verbose=verbose,
+            random_state=random_state,
+            tol=tol,
+            n_init=n_init,
+            copy_x=copy_x,
+        )
+        self.delta = delta
+        self.max_clusters = max_clusters
+
+    def fit(self, X, y=None, sample_weight=None):
+        """Compute dp-means clustering.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training instances to cluster. It must be noted that the data
+            will be converted to C ordering, which will cause a memory
+            copy if the given data is not C-contiguous.
+            If a sparse matrix is passed, a copy will be made if it's not in
+            CSR format.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            The weights for each observation in X. If None, all observations
+            are assigned equal weight.
+
+            .. versionadded:: 0.20
+
+        Returns
+        -------
+        self : object
+            Fitted estimator.
+        """
+        X = self._validate_data(
+            X,
+            accept_sparse="csr",
+            dtype=[np.float64, np.float32],
+            order="C",
+            copy=self.copy_x,
+            accept_large_sparse=False,
+        )
+
+        super()._check_params_vs_input(X)
+        random_state = check_random_state(self.random_state)
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+        self._n_threads = _openmp_effective_n_threads()
+
+        # Validate init array
+        init = self.init
+        if hasattr(init, "__array__"):
+            init = check_array(init, dtype=X.dtype, copy=True, order="C")
+            self._validate_center_shape(X, init)
+
+        # subtract of mean of x for more accurate distance computations
+        if not sp.issparse(X):
+            X_mean = X.mean(axis=0)
+            # The copy was already done above
+            X -= X_mean
+
+            if hasattr(init, "__array__"):
+                init -= X_mean
+
+        # precompute squared norms of data points
+        x_squared_norms = row_norms(X, squared=True)
+
+        dpmeans_single = _dpmeans_single_lloyd
+        self._check_mkl_vcomp(X, X.shape[0])
+
+        best_inertia, best_labels = None, None
+
+        for i in range(self._n_init):
+            # Initialize centers
+            centers_init = self._init_centroids(
+                X,
+                x_squared_norms=x_squared_norms,
+                init=init,
+                random_state=random_state,
+            )
+
+            if self.verbose:
+                print("Initialization complete")
+
+            # run a dp-means once
+
+            labels, inertia, centers, n_iter_, all_centers, iter_time = dpmeans_single(
+                X,
+                sample_weight,
+                centers_init,
+                max_iter=self.max_iter,
+                verbose=self.verbose,
+                tol=self._tol,
+                x_squared_norms=x_squared_norms,
+                n_threads=self._n_threads,
+                delta=self.delta,
+                max_clusters=self.max_clusters,
+            )
+
+            inertia += self.delta * centers.shape[0]
+
+            # determine if these results are the best so far
+            # we chose a new run if it has a better inertia and the clustering is
+            # different from the best so far (it's possible that the inertia is
+            # slightly better even if the clustering is the same with potentially
+            # permuted labels, due to rounding errors)
+            if best_inertia is None or (inertia < best_inertia):
+                best_labels = labels
+                best_centers = centers
+                best_inertia = inertia
+                best_n_iter = n_iter_
+
+        if not sp.issparse(X):
+            if not self.copy_x:
+                X += X_mean
+            best_centers += X_mean
+        self.n_clusters = best_centers.shape[0]
+        self.cluster_centers_ = best_centers
+        self.labels_ = best_labels
+        self.inertia_ = best_inertia
+        self.n_iter_ = best_n_iter
+        # self.iter_time = iter_time
+        # self.all_centers = all_centers
+        return self
+
+
+def _labels_inertia_with_min_sample(
+    X, sample_weight, x_squared_norms, centers, n_threads=1
+):
+    """E step of the K-means EM algorithm.
+
+    Compute the labels and the inertia of the given samples and centers.
+
+    Parameters
+    ----------
+    X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+        The input samples to assign to the labels. If sparse matrix, must
+        be in CSR format.
+
+    sample_weight : ndarray of shape (n_samples,)
+        The weights for each observation in X.
+
+    x_squared_norms : ndarray of shape (n_samples,)
+        Precomputed squared euclidean norm of each data point, to speed up
+        computations.
+
+    centers : ndarray of shape (n_clusters, n_features)
+        The cluster centers.
+
+    n_threads : int, default=1
+        The number of OpenMP threads to use for the computation. Parallelism is
+        sample-wise on the main cython loop which assigns each sample to its
+        closest center.
+
+    Returns
+    -------
+    labels : ndarray of shape (n_samples,)
+        The resulting assignment.
+
+    inertia : float
+        Sum of squared distances of samples to their closest cluster center.
+    """
+    n_samples = X.shape[0]
+    n_clusters = centers.shape[0]
+
+    labels = np.full(n_samples, -1, dtype=np.int32)
+    max_index = np.full(1, -1, dtype=np.int32)
+    max_distance = np.full(1, -1, dtype=centers.dtype)
+    weight_in_clusters = np.zeros(n_clusters, dtype=centers.dtype)
+    center_shift = np.zeros_like(weight_in_clusters)
+
+    _labels = lloyd_iter_chunked_dense_with_min_sample
+    _inertia = _inertia_dense
+    X = X
+
+    _labels(
+        X,
+        sample_weight,
+        x_squared_norms,
+        centers,
+        centers,
+        weight_in_clusters,
+        labels,
+        center_shift,
+        max_index,
+        max_distance,
+        n_threads,
+        update_centers=False,
+    )
+
+    inertia = _inertia(X, sample_weight, centers, labels, n_threads)
+
+    return labels, inertia, max_index, max_distance
+
+
+def _mini_batch_step_with_max_distance(
+    X,
+    x_squared_norms,
+    sample_weight,
+    centers,
+    centers_new,
+    weight_sums,
+    random_state,
+    random_reassign=False,
+    reassignment_ratio=0.01,
+    verbose=False,
+    n_threads=1,
+):
+    """Incremental update of the centers for the Minibatch K-Means algorithm.
+
+    Parameters
+    ----------
+
+    X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+        The original data array. If sparse, must be in CSR format.
+
+    x_squared_norms : ndarray of shape (n_samples,)
+        Squared euclidean norm of each data point.
+
+    sample_weight : ndarray of shape (n_samples,)
+        The weights for each observation in X.
+
+    centers : ndarray of shape (n_clusters, n_features)
+        The cluster centers before the current iteration
+
+    centers_new : ndarray of shape (n_clusters, n_features)
+        The cluster centers after the current iteration. Modified in-place.
+
+    weight_sums : ndarray of shape (n_clusters,)
+        The vector in which we keep track of the numbers of points in a
+        cluster. This array is modified in place.
+
+    random_state : RandomState instance
+        Determines random number generation for low count centers reassignment.
+        See :term:`Glossary <random_state>`.
+
+    random_reassign : boolean, default=False
+        If True, centers with very low counts are randomly reassigned
+        to observations.
+
+    reassignment_ratio : float, default=0.01
+        Control the fraction of the maximum number of counts for a
+        center to be reassigned. A higher value means that low count
+        centers are more likely to be reassigned, which means that the
+        model will take longer to converge, but should converge in a
+        better clustering.
+
+    verbose : bool, default=False
+        Controls the verbosity.
+
+    n_threads : int, default=1
+        The number of OpenMP threads to use for the computation.
+
+    Returns
+    -------
+    inertia : float
+        Sum of squared distances of samples to their closest cluster center.
+        The inertia is computed after finding the labels and before updating
+        the centers.
+    """
+    # Perform label assignment to nearest centers
+    # For better efficiency, it's better to run _mini_batch_step in a
+    # threadpool_limit context than using _labels_inertia_threadpool_limit here
+    labels, inertia, max_index, max_distance = _labels_inertia_with_min_sample(
+        X, sample_weight, x_squared_norms, centers, n_threads=n_threads
+    )
+
+    # Update centers according to the labels
+
+    _minibatch_update_dense(
+        X,
+        sample_weight,
+        centers,
+        centers_new,
+        weight_sums,
+        labels,
+        n_threads,
+    )
+
+    # Reassign clusters that have very low weight
+
+    if random_reassign and reassignment_ratio > 0:
+        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()
+
+        # pick at most .5 * batch_size samples as new centers
+        if to_reassign.sum() > 0.5 * X.shape[0]:
+            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]
+            to_reassign[indices_dont_reassign] = False
+        n_reassigns = to_reassign.sum()
+
+        if n_reassigns:
+            # Pick new clusters amongst observations with uniform probability
+            new_centers = random_state.choice(
+                X.shape[0], replace=False, size=n_reassigns
+            )
+            if verbose:
+                print(f"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.")
+
+            if sp.issparse(X):
+                assign_rows_csr(
+                    X,
+                    new_centers.astype(np.intp, copy=False),
+                    np.where(to_reassign)[0].astype(np.intp, copy=False),
+                    centers_new,
+                )
+            else:
+                centers_new[to_reassign] = X[new_centers]
+
+        # reset counts of reassigned centers, but don't reset them too small
+        # to avoid instant reassignment. This is a pretty dirty hack as it
+        # also modifies the learning rates.
+        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])
+
+    return inertia, max_index[0], max_distance[0]
+
+
+class MiniBatchDPMeans(KMeans):
+    """
+    Parameters
+    ----------
+
+    n_clusters : int, default=1
+        The initial number of clusters to form as well as the number of
+        centroids to generate.
+
+    init : {'k-means++', 'random'}, callable or array-like of shape \
+            (n_clusters, n_features), default='k-means++'
+        Method for initialization:
+
+        'k-means++' : selects initial cluster centroids for deterministic 
+        initialization using sampling based on an empirical probability distribution 
+        of the points' contribution to the overall inertia. This technique 
+        speeds up convergence. The algorithm implemented is "greedy k-means++". 
+        It differs from the vanilla k-means++ by making several trials at each 
+        sampling step and choosing the best centroid among them.
+
+        'random': choose `n_clusters` observations (rows) at random from data
+        for the initial centroids.
+
+        If an array is passed, it should be of shape (n_clusters, n_features)
+        and gives the initial centers.
+
+        If a callable is passed, it should take arguments X, n_clusters and a
+        random state and return an initialization.
+
+    max_iter : int, default=100
+        Maximum number of iterations over the complete dataset before
+        stopping independently of any early stopping criterion heuristics.
+
+    batch_size : int, default=1024
+        Size of the mini batches.
+        For faster computations, you can set the ``batch_size`` greater than
+        256 * number of cores to enable parallelism on all cores.
+
+    verbose : int, default=0
+        Verbosity mode.
+
+    compute_labels : bool, default=True
+        Compute label assignment and inertia for the complete dataset
+        once the minibatch optimization has converged in fit.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines random number generation for centroid initialization and
+        random reassignment. Use an int to make the randomness deterministic.
+        See :term:`Glossary <random_state>`.
+
+    tol : float, default=0.0
+        Control early stopping based on the relative center changes as
+        measured by a smoothed, variance-normalized of the mean center
+        squared position changes.
+
+    max_no_improvement : int, default=10
+        Control early stopping based on the consecutive number of mini
+        batches that does not yield an improvement on the smoothed inertia.
+
+    init_size : int, default=None
+        Number of samples to randomly sample for speeding up the
+        initialization (sometimes at the expense of accuracy): the
+        only algorithm is initialized by running a batch DPMeans on a
+        random subset of the data. This needs to be larger than n_clusters.
+
+    n_init : int, default=3
+        Number of random initializations that are tried.
+        The algorithm is only run once, using the best of
+        the `n_init` initializations as measured by inertia.
+
+    reassignment_ratio : float, default=0.01
+        Control the fraction of the maximum number of counts for a center to
+        be reassigned. A higher value means that low count centers are more
+        easily reassigned, which means that the model will take longer to
+        converge, but should converge in a better clustering.
+
+    delta : float, default=1.0
+        Parameter controlling the number of clusters in the DP-means
+        algorithm. A higher value will lead to fewer clusters.
+
+    Attributes
+    ----------
+        cluster_centers_ : ndarray of shape (n_clusters, n_features)
+            Coordinates of cluster centers.
+
+        labels_ : ndarray of shape (n_samples,)
+            Labels of each point (if compute_labels is set to True).
+
+        inertia_ : float
+            The value of the inertia criterion associated with the chosen
+            partition if compute_labels is set to True. If compute_labels is set to
+            False, it's an approximation of the inertia based on an exponentially
+            weighted average of the batch inertiae.
+            The inertia is defined as the sum of square distances of samples to
+            their cluster center, weighted by the sample weights if provided.
+
+        n_iter_ : int
+            Number of iterations over the full dataset.
+
+        n_features_in_ : int
+            Number of features seen during :term:`fit`.
+
+        feature_names_in_ : ndarray of shape (`n_features_in_`,)
+            Names of features seen during :term:`fit`. Defined only when `X`
+            has feature names that are all strings.
+
+        See Also
+        --------
+        DPMeans : The full batch version of DP-Means clustering.
+
+        KMeans : The classic implementation of the clustering method based on the
+            Lloyd's algorithm. It consumes the whole set of input data at each
+            iteration.
+
+        Notes
+        -----
+        When there are too few points in the dataset, some centers may be
+        duplicated, which means that a proper clustering in terms of the number
+        of requesting clusters and the number of returned clusters will not
+        always match. One solution is to set `reassignment_ratio=0`, which
+        prevents reassignments of clusters that are too small.
+
+        Examples
+        --------
+        >>> from pdc_dp_means import MiniBatchDPMeans
+        >>> import numpy as np
+        >>> X = np.array([[1, 2], [1, 4], [1, 0],
+        ...               [4, 2], [4, 0], [4, 4],
+        ...               [4, 5], [0, 1], [2, 2],
+        ...               [3, 2], [5, 5], [1, -1]])
+        >>> # manually fit on batches
+        >>> dpmeans = MiniBatchDPMeans(n_clusters=1,
+        ...                            random_state=0,
+        ...                            batch_size=6,
+        ...                            n_init=3,
+        ...                            delta=1.0)
+        >>> dpmeans = dpmeans.partial_fit(X[0:6,:])
+        >>> dpmeans = dpmeans.partial_fit(X[6:12,:])
+        >>> dpmeans.cluster_centers_
+        array([[3.375, 3.  ],
+            [0.75 , 0.5 ]])
+        >>> dpmeans.predict([[0, 0], [4, 4]])
+        array([1, 0], dtype=int32)
+        >>> # fit on the whole data
+        >>> dpmeans = MiniBatchDPMeans(n_clusters=1,
+        ...                            random_state=0,
+        ...                            batch_size=6,
+        ...                            max_iter=10,
+        ...                            n_init=3,
+        ...                            delta=1.0).fit(X)
+        >>> dpmeans.cluster_centers_
+        array([[3.55102041, 2.48979592],
+            [1.06896552, 1.        ]])
+        >>> dpmeans.predict([[0, 0], [4, 4]])
+        array([1, 0], dtype=int32)
+
+    
+    """
+
+    def __init__(
+        self,
+        n_clusters=1,
+        *,
+        init="k-means++",
+        max_iter=100,
+        batch_size=1024,
+        verbose=0,
+        compute_labels=True,
+        random_state=None,
+        tol=0.0,
+        max_no_improvement=10,
+        init_size=None,
+        n_init=3,
+        reassignment_ratio=0.01,
+        delta=1.0,
+    ):
+        super().__init__(
+            n_clusters=n_clusters,
+            init=init,
+            max_iter=max_iter,
+            verbose=verbose,
+            random_state=random_state,
+            tol=tol,
+            n_init=n_init,
+        )
+
+        self.max_no_improvement = max_no_improvement
+        self.batch_size = batch_size
+        self.compute_labels = compute_labels
+        self.init_size = init_size
+        self.reassignment_ratio = reassignment_ratio
+        self.delta = delta
+
+    @deprecated(  # type: ignore
+        "The attribute `counts_` is deprecated in 0.24"
+        " and will be removed in 1.1 (renaming of 0.26)."
+    )
+    @property
+    def counts_(self):
+        return self._counts
+
+    @deprecated(  # type: ignore
+        "The attribute `init_size_` is deprecated in "
+        "0.24 and will be removed in 1.1 (renaming of 0.26)."
+    )
+    @property
+    def init_size_(self):
+        return self._init_size
+
+    @deprecated(  # type: ignore
+        "The attribute `random_state_` is deprecated "
+        "in 0.24 and will be removed in 1.1 (renaming of 0.26)."
+    )
+    @property
+    def random_state_(self):
+        return getattr(self, "_random_state", None)
+
+    def _check_params(self, X):
+        super()._check_params_vs_input(X)
+
+        # max_no_improvement
+        if self.max_no_improvement is not None and self.max_no_improvement < 0:
+            raise ValueError(
+                "max_no_improvement should be >= 0, got "
+                f"{self.max_no_improvement} instead."
+            )
+
+        # batch_size
+        if self.batch_size <= 0:
+            raise ValueError(
+                f"batch_size should be > 0, got {self.batch_size} instead."
+            )
+        self._batch_size = min(self.batch_size, X.shape[0])
+
+        # init_size
+        if self.init_size is not None and self.init_size <= 0:
+            raise ValueError(f"init_size should be > 0, got {self.init_size} instead.")
+        self._init_size = self.init_size
+        if self._init_size is None:
+            self._init_size = 3 * self._batch_size
+            if self._init_size < self.n_clusters:
+                self._init_size = 3 * self.n_clusters
+        elif self._init_size < self.n_clusters:
+            self._init_size = 3 * self.n_clusters
+        self._init_size = min(self._init_size, X.shape[0])
+
+        # reassignment_ratio
+        if self.reassignment_ratio < 0:
+            raise ValueError(
+                "reassignment_ratio should be >= 0, got "
+                f"{self.reassignment_ratio} instead."
+            )
+
+    def _mini_batch_convergence(
+        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia
+    ):
+        """Helper function to encapsulate the early stopping logic"""
+        # Normalize inertia to be able to compare values when
+        # batch_size changes
+        batch_inertia /= self._batch_size
+
+        # count steps starting from 1 for user friendly verbose mode.
+        step = step + 1
+
+        # Ignore first iteration because it's inertia from initialization.
+        if step == 1:
+            if self.verbose:
+                print(
+                    f"Minibatch step {step}/{n_steps}: mean batch "
+                    f"inertia: {batch_inertia}"
+                )
+            return False
+
+        # Compute an Exponentially Weighted Average of the inertia to
+        # monitor the convergence while discarding minibatch-local stochastic
+        # variability: https://en.wikipedia.org/wiki/Moving_average
+        if self._ewa_inertia is None:
+            self._ewa_inertia = batch_inertia
+        else:
+            alpha = self._batch_size * 2.0 / (n_samples + 1)
+            alpha = min(alpha, 1)
+            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha
+
+        # Log progress to be able to monitor convergence
+        if self.verbose:
+            print(
+                f"Minibatch step {step}/{n_steps}: mean batch inertia: "
+                f"{batch_inertia}, ewa inertia: {self._ewa_inertia}"
+            )
+
+        # Early stopping based on absolute tolerance on squared change of
+        # centers position
+        if self._tol > 0.0 and centers_squared_diff <= self._tol:
+            if self.verbose:
+                print(f"Converged (small centers change) at step {step}/{n_steps}")
+            return True
+
+        # Early stopping heuristic due to lack of improvement on smoothed
+        # inertia
+        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:
+            self._no_improvement = 0
+            self._ewa_inertia_min = self._ewa_inertia
+        else:
+            self._no_improvement += 1
+
+        if (
+            self.max_no_improvement is not None
+            and self._no_improvement >= self.max_no_improvement
+        ):
+            if self.verbose:
+                print(
+                    "Converged (lack of improvement in inertia) at step "
+                    f"{step}/{n_steps}"
+                )
+            return True
+
+        return False
+
+    def _random_reassign(self):
+        """Check if a random reassignment needs to be done.
+
+        Do random reassignments each time 10 * n_clusters samples have been
+        processed.
+
+        If there are empty clusters we always want to reassign.
+        """
+        self._n_since_last_reassign += self._batch_size
+        if (self._counts == 0).any() or self._n_since_last_reassign >= (
+            10 * self.n_clusters
+        ):
+            self._n_since_last_reassign = 0
+            return True
+        return False
+
+    def fit(self, X, y=None, sample_weight=None):
+        """Compute the centroids on X by chunking it into mini-batches.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training instances to cluster. It must be noted that the data
+            will be converted to C ordering, which will cause a memory copy
+            if the given data is not C-contiguous.
+            If a sparse matrix is passed, a copy will be made if it's not in
+            CSR format.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            The weights for each observation in X. If None, all observations
+            are assigned equal weight.
+
+            .. versionadded:: 0.20
+
+        Returns
+        -------
+        self : object
+            Fitted estimator.
+        """
+        X = self._validate_data(
+            X,
+            accept_sparse="csr",
+            dtype=[np.float64, np.float32],
+            order="C",
+            accept_large_sparse=False,
+        )
+
+        self._check_params(X)
+        random_state = check_random_state(self.random_state)
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+        self._n_threads = _openmp_effective_n_threads()
+        n_samples, n_features = X.shape
+
+        # Validate init array
+        init = self.init
+        if hasattr(init, "__array__"):
+            init = check_array(init, dtype=X.dtype, copy=True, order="C")
+            self._validate_center_shape(X, init)
+
+        self._check_mkl_vcomp(X, self._batch_size)
+
+        # precompute squared norms of data points
+        x_squared_norms = row_norms(X, squared=True)
+
+        # Validation set for the init
+        validation_indices = random_state.randint(0, n_samples, self._init_size)
+        X_valid = X[validation_indices]
+        sample_weight_valid = sample_weight[validation_indices]
+
+        # perform several inits with random subsets
+        best_inertia = None
+        for init_idx in range(self._n_init):
+            if self.verbose:
+                print(f"Init {init_idx + 1}/{self._n_init} with method {init}")
+
+            # Initialize the centers using only a fraction of the data as we
+            # expect n_samples to be very large when using MiniBatchKMeans.
+            cluster_centers = self._init_centroids(
+                X,
+                x_squared_norms=x_squared_norms,
+                init=init,
+                random_state=random_state,
+                init_size=self._init_size,
+            )
+
+            # Compute inertia on a validation set.
+            _, inertia = _labels_inertia_threadpool_limit(
+                X_valid,
+                sample_weight_valid,
+                cluster_centers,
+                n_threads=self._n_threads,
+            )
+
+            if self.verbose:
+                print(f"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}")
+            if best_inertia is None or inertia < best_inertia:
+                init_centers = cluster_centers
+                best_inertia = inertia
+
+        centers = init_centers
+        centers_new = np.empty_like(centers)
+
+        # Initialize counts
+        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)
+
+        # Attributes to monitor the convergence
+        self._ewa_inertia = None
+        self._ewa_inertia_min = None
+        self._no_improvement = 0
+
+        # Initialize number of samples seen since last reassignment
+        self._n_since_last_reassign = 0
+
+        n_steps = (self.max_iter * n_samples) // self._batch_size
+
+        iter_time = [0]
+        self.iter_centers = [None]
+
+        with threadpool_limits(limits=1, user_api="blas"):
+            # Perform the iterative optimization until convergence
+            for i in range(n_steps):
+                # Sample a minibatch from the full dataset
+                minibatch_indices = random_state.randint(
+                    0, n_samples, self._batch_size
+                )
+                tic = time()
+                # Perform the actual update step on the minibatch data
+                (
+                    batch_inertia,
+                    max_index,
+                    max_distance,
+                ) = _mini_batch_step_with_max_distance(
+                    X=X[minibatch_indices],
+                    x_squared_norms=x_squared_norms[minibatch_indices],
+                    sample_weight=sample_weight[minibatch_indices],
+                    centers=centers,
+                    centers_new=centers_new,
+                    weight_sums=self._counts,
+                    random_state=random_state,
+                    random_reassign=self._random_reassign(),
+                    reassignment_ratio=self.reassignment_ratio,
+                    verbose=self.verbose,
+                    n_threads=self._n_threads,
+                )
+                toc = time() - tic
+                iter_time.append(toc)
+                new_cluster = False
+                if max_index != -1 and max_distance > self.delta:
+                    centers = np.vstack(
+                        (centers, X[minibatch_indices[max_index]])
+                    ).astype(X.dtype)
+                    centers_new = np.vstack(
+                        (centers_new, X[minibatch_indices[max_index]])
+                    ).astype(X.dtype)
+                    self.n_clusters += 1
+                    self._counts = np.hstack([self._counts, [1]]).astype(X.dtype)
+                    new_cluster = True
+
+                if self._tol > 0.0:
+                    centers_squared_diff = np.sum((centers_new - centers) ** 2)
+                else:
+                    centers_squared_diff = 0
+                centers, centers_new = centers_new, centers
+                self.iter_centers.append(centers)
+                # Monitor convergence and do early stopping if necessary
+                if new_cluster is False and self._mini_batch_convergence(
+                    i, n_steps, n_samples, centers_squared_diff, batch_inertia
+                ):
+                    break
+
+        self.cluster_centers_ = centers
+        self.iter_time = iter_time
+        self.n_steps_ = i + 1
+        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))
+
+        if self.compute_labels:
+            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(
+                X,
+                sample_weight,
+                self.cluster_centers_,
+                n_threads=self._n_threads,
+            )
+            self.inertia_ += self.delta * self.cluster_centers_.shape[0]
+        else:
+            self.inertia_ = (
+                self._ewa_inertia * n_samples
+                + self.delta * self.cluster_centers_.shape[0]
+            )
+
+        return self
+
+    def partial_fit(self, X, y=None, sample_weight=None):
+        """Update k means estimate on a single mini-batch X.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training instances to cluster. It must be noted that the data
+            will be converted to C ordering, which will cause a memory copy
+            if the given data is not C-contiguous.
+            If a sparse matrix is passed, a copy will be made if it's not in
+            CSR format.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            The weights for each observation in X. If None, all observations
+            are assigned equal weight.
+
+        Returns
+        -------
+        self : object
+            Return updated estimator.
+        """
+        has_centers = hasattr(self, "cluster_centers_")
+
+        X = self._validate_data(
+            X,
+            accept_sparse="csr",
+            dtype=[np.float64, np.float32],
+            order="C",
+            accept_large_sparse=False,
+            reset=not has_centers,
+        )
+
+        self._random_state = getattr(
+            self, "_random_state", check_random_state(self.random_state)
+        )
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+        self.n_steps_ = getattr(self, "n_steps_", 0)
+
+        # precompute squared norms of data points
+        x_squared_norms = row_norms(X, squared=True)
+
+        if not has_centers:
+            # this instance has not been fitted yet (fit or partial_fit)
+            self._check_params(X)
+            self._n_threads = _openmp_effective_n_threads()
+
+            # Validate init array
+            init = self.init
+            if hasattr(init, "__array__"):
+                init = check_array(init, dtype=X.dtype, copy=True, order="C")
+                self._validate_center_shape(X, init)
+
+            self._check_mkl_vcomp(X, X.shape[0])
+
+            # initialize the cluster centers
+            self.cluster_centers_ = self._init_centroids(
+                X,
+                x_squared_norms=x_squared_norms,
+                init=init,
+                random_state=self._random_state,
+                init_size=self._init_size,
+            )
+
+            # Initialize counts
+            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)
+
+            # Initialize number of samples seen since last reassignment
+            self._n_since_last_reassign = 0
+
+        with threadpool_limits(limits=1, user_api="blas"):
+            _, max_index, max_distance = _mini_batch_step_with_max_distance(
+                X,
+                x_squared_norms=x_squared_norms,
+                sample_weight=sample_weight,
+                centers=self.cluster_centers_,
+                centers_new=self.cluster_centers_,
+                weight_sums=self._counts,
+                random_state=self._random_state,
+                random_reassign=self._random_reassign(),
+                reassignment_ratio=self.reassignment_ratio,
+                verbose=self.verbose,
+                n_threads=self._n_threads,
+            )
+
+            if max_index != -1 and max_distance > self.delta:
+                self.cluster_centers_ = np.vstack(
+                    (self.cluster_centers_, X[max_index])
+                ).astype(X.dtype)
+                self.n_clusters += 1
+                self._counts = np.hstack([self._counts, [1]]).astype(X.dtype)
+
+        if self.compute_labels:
+            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(
+                X,
+                sample_weight,
+                self.cluster_centers_,
+                n_threads=self._n_threads,
+            )
+            self.inertia_ += self.delta * self.cluster_centers_.shape[0]
+
+        self.n_steps_ += 1
+
+        return self
+
+    def predict(self, X, sample_weight=None):
+        """Predict the closest cluster each sample in X belongs to.
+
+        In the vector quantization literature, `cluster_centers_` is called
+        the code book and each value returned by `predict` is the index of
+        the closest code in the code book.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            New data to predict.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            The weights for each observation in X. If None, all observations
+            are assigned equal weight.
+
+        Returns
+        -------
+        labels : ndarray of shape (n_samples,)
+            Index of the cluster each sample belongs to.
+        """
+        check_is_fitted(self)
+
+        X = self._check_test_data(X)
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+
+        labels, _ = _labels_inertia_threadpool_limit(
+            X,
+            sample_weight,
+            self.cluster_centers_,
+            n_threads=self._n_threads,
+        )
+
+        return labels
+
+    def _more_tags(self):
+        return {
+            "_xfail_checks": {
+                "check_sample_weights_invariance": (
+                    "zero sample_weight is not equivalent to removing samples"
+                ),
+            }
+        }
```

## pdc_dp_means/release.py

```diff
@@ -1 +1 @@
-__version__ = "0.0.3"
+__version__ = "0.0.4"
```

## Comparing `pdc_dp_means-0.0.3.dist-info/LICENSE` & `pdc_dp_means-0.0.4.dist-info/LICENSE`

 * *Files 15% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-BSD 3-Clause License
-
-Copyright (c) 2022, BGU-CS-VIL
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-1. Redistributions of source code must retain the above copyright notice, this
-   list of conditions and the following disclaimer.
-
-2. Redistributions in binary form must reproduce the above copyright notice,
-   this list of conditions and the following disclaimer in the documentation
-   and/or other materials provided with the distribution.
-
-3. Neither the name of the copyright holder nor the names of its
-   contributors may be used to endorse or promote products derived from
-   this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+BSD 3-Clause License
+
+Copyright (c) 2022, Or Dinari
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

## Comparing `pdc_dp_means-0.0.3.dist-info/METADATA` & `pdc_dp_means-0.0.4.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,86 +1,94 @@
-Metadata-Version: 2.1
-Name: pdc-dp-means
-Version: 0.0.3
-Author: Or Dinari
-Author-email: dinari.or@gmail.com
-License: BSD3
-Project-URL: Source, https://github.com/BGU-CS-VIL/pdc-dp-means
-Project-URL: Tracker, https://github.com/BGU-CS-VIL/pdc-dp-means
-Project-URL: Documentation, https://pdc-dp-means.readthedocs.io/en/latest/
-Keywords: dp-means clustering
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: BSD License
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy
-Requires-Dist: scikit-learn
-
-# Revisiting DP-Means: Fast Scalable Algorithms via Parallelism and Delayed Cluster Creation
-[Paper](https://openreview.net/pdf?id=rnzVBD8jqlq) <br>
-
-## Introduction
-DP-means (Kulis and Jordan, ICML 2012), a nonparametric generalization of K-means, extends the latter to the case where the
-number of clusters is unknown. Unlike K-means, however, DP-means is hard to parallelize, a limitation hindering its usage in large-scale tasks. This work bridges this practicality gap by rendering the DP-means approach a viable, fast, and highly-scalable solution. In our paper, we first study the strengths and weaknesses of previous attempts to parallelize the DP-means algorithm. Next, we propose a new parallel algorithm, called PDC-DP-Means (Parallel Delayed Cluster DP-Means), based in part on delayed creation of clusters. Compared with DP-Means, PDC-DP-Means provides not only a major speedup but also performance gains. Finally, we propose two extensions of PDC-DP-Means. The first combines it with an existing method, leading to further speedups. The second extends PDC-DP-Means to a Mini-Batch setting (with an optional support for an online mode), allowing for another major speedup. We verify the utility of the pro-posed methods on multiple datasets. We also show that the proposed methods outperform other non-parametric methods (e.g., DBSCAN). Our highly-efficient code, available in this git repository, can be used to reproduce our experiments. 
-
-## Code
-
-
-The supplied code has 3 parts -
-
-* The cluster directory, which contains an extension to sklearn with our proposed algorithms, PDC-DP-Means and its MiniBatch version.
-* the file `date_pdpmeans.py` which contains our implementation of DACE (in three versions, see below) and PDP-Means.
-* Three notebooks that contain the experiment with the other non-parametric methods.
-
-### PDC-DP-Means and MiniBatch PDC-DP-Means
-
-In order to install this, you must clone scikit-learn from: `https://github.com/scikit-learn/scikit-learn.git`.
-
-Navigate to the directory `sklearn/cluster` and replace the files `__init__.py`, `_k_means_lloyd.pyx` and `_kmeans.py` with the respective files under the `cluster` directory.
-Next, you need to install sklearn from source. To do so, follow the directions here: https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge.
-
-Now, in order to use it, you can simply use `from sklearn.cluster import MiniBatchDPMeans, DPMeans`. In general, the parameters are the same as the `K-Means` counterpart:
-https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
-
-https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html
-
-The only differences are:
-1) instead of the `n_clusters` parameter (which stands, in K-Means, for the number fo clusters), there is a new parameter called `delta` (in our papers it was lambda but avoided this vairable name here since lambda is a reserved word in Python);
-2) When DPMeans is used the `algorithm` parameter is removed.
-
-### DACE and PDP-Means
-
-In the file `dace_dpmeans.py` there are 4 relevant algorithms -
-
-`parallel_dp(data,delta,processes,iters)' - PDP-Means.  As before, delta replaces lambda, `data' is the data, 'processes' is the amount of parallelization, and `iters' is the maximum iterations (it will stop before if converged).
-
-`DACE(data,delta,num_of_processes)` - The original DACE algorithm. as before, delta replaces lambda, 'data' is the data, num_of_processes is the amount of parallelization.
-
-`DACE_DDP(data,delta,num_of_processes)` - DACE using PDC-DP-Means, but with no inner parallelization.
-
-`DACE_DDP_SPAWN(data,delta,num_of_processes)` - DACE using PDP-DP-Means with inner parallelization, due to different Multi Processing scheme, this might take abit longer to start.
-
-
-Note that in order to run this file some extra dependencies are required, `evaluations.py` file contain several functions, and while some packages required are quite standard - `torchvision,scikit-learn,annoy,pandas,numpy`, it is also required to have a valid `R` enviroment, and the `R` package `maotai` installed, and the python-R interface package `rpy2`.
-
-
-### Experiment notebooks
-We have included the experiments which does not require additional installations apart from the build-from-source scikit-learn, the three attached notebooks are used to recreate the experiments with the other non-parametric methods. Note that the blackbox optimization (while we supplied the code to run it), need to run separately, as it's multiprocess does not play well with Jupyter Notebook. 
-
-
-### Citing this work
-If you use this code for your work, please cite the following:
-
-```
-@inproceedings{dinari2022revisiting,
-  title={Revisiting DP-Means: Fast Scalable Algorithms via Parallelism and Delayed Cluster Creation},
-  author={Dinari, Or and Freifeld, Oren},
-  booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
-  year={2022}
-}
-```
-### License 
-While our additional code is under the GPL license, some code snippets here are taken from sklearn, and using them (e.g., the code under the `cluster` directory) is subject to sklearn's license.
+Metadata-Version: 2.1
+Name: pdc-dp-means
+Version: 0.0.4
+Author: Or Dinari
+Author-email: dinari.or@gmail.com
+License: BSD3
+Project-URL: Source, https://github.com/BGU-CS-VIL/pdc-dp-means
+Project-URL: Tracker, https://github.com/BGU-CS-VIL/pdc-dp-means
+Project-URL: Documentation, https://pdc-dp-means.readthedocs.io/en/latest/
+Keywords: dp-means clustering
+Classifier: Development Status :: 4 - Beta
+Classifier: License :: OSI Approved :: BSD License
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
+# Revisiting DP-Means: Fast Scalable Algorithms via Parallelism and Delayed Cluster Creation
+[Paper](https://openreview.net/pdf?id=rnzVBD8jqlq) <br>
+
+### Introduction
+DP-means (Kulis and Jordan, ICML 2012), a nonparametric generalization of K-means, extends the latter to the case where the
+number of clusters is unknown. Unlike K-means, however, DP-means is hard to parallelize, a limitation hindering its usage in large-scale tasks. This work bridges this practicality gap by rendering the DP-means approach a viable, fast, and highly-scalable solution. In our paper, we first study the strengths and weaknesses of previous attempts to parallelize the DP-means algorithm. Next, we propose a new parallel algorithm, called PDC-DP-Means (Parallel Delayed Cluster DP-Means), based in part on delayed creation of clusters. Compared with DP-Means, PDC-DP-Means provides not only a major speedup but also performance gains. Finally, we propose two extensions of PDC-DP-Means. The first combines it with an existing method, leading to further speedups. The second extends PDC-DP-Means to a Mini-Batch setting (with an optional support for an online mode), allowing for another major speedup. We verify the utility of the pro-posed methods on multiple datasets. We also show that the proposed methods outperform other non-parametric methods (e.g., DBSCAN). Our highly-efficient code, available in this git repository, can be used to reproduce our experiments. 
+
+
+### Installation
+`pip install pdc-dp-means`
+
+### Usage
+Please refer to the documentation: https://pdc-dp-means.readthedocs.io/en/latest/
+
+
+
+
+## Code
+
+The code described here is under the folder `paper_code`.
+The supplied code has 3 parts -
+
+* The cluster directory, which contains an extension to sklearn with our proposed algorithms, PDC-DP-Means and its MiniBatch version.
+* the file `date_pdpmeans.py` which contains our implementation of DACE (in three versions, see below) and PDP-Means.
+* Three notebooks that contain the experiment with the other non-parametric methods.
+
+### PDC-DP-Means and MiniBatch PDC-DP-Means
+
+In order to install this, you must clone scikit-learn from: `https://github.com/scikit-learn/scikit-learn.git`.
+
+Navigate to the directory `sklearn/cluster` and replace the files `__init__.py`, `_k_means_lloyd.pyx` and `_kmeans.py` with the respective files under the `cluster` directory.
+Next, you need to install sklearn from source. To do so, follow the directions here: https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge.
+
+Now, in order to use it, you can simply use `from sklearn.cluster import MiniBatchDPMeans, DPMeans`. In general, the parameters are the same as the `K-Means` counterpart:
+https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
+
+https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html
+
+The only differences are:
+1) instead of the `n_clusters` parameter (which stands, in K-Means, for the number fo clusters), there is a new parameter called `delta` (in our papers it was lambda but avoided this vairable name here since lambda is a reserved word in Python);
+2) When DPMeans is used the `algorithm` parameter is removed.
+
+### DACE and PDP-Means
+
+In the file `dace_dpmeans.py` there are 4 relevant algorithms -
+
+`parallel_dp(data,delta,processes,iters)' - PDP-Means.  As before, delta replaces lambda, `data' is the data, 'processes' is the amount of parallelization, and `iters' is the maximum iterations (it will stop before if converged).
+
+`DACE(data,delta,num_of_processes)` - The original DACE algorithm. as before, delta replaces lambda, 'data' is the data, num_of_processes is the amount of parallelization.
+
+`DACE_DDP(data,delta,num_of_processes)` - DACE using PDC-DP-Means, but with no inner parallelization.
+
+`DACE_DDP_SPAWN(data,delta,num_of_processes)` - DACE using PDP-DP-Means with inner parallelization, due to different Multi Processing scheme, this might take abit longer to start.
+
+
+Note that in order to run this file some extra dependencies are required, `evaluations.py` file contain several functions, and while some packages required are quite standard - `torchvision,scikit-learn,annoy,pandas,numpy`, it is also required to have a valid `R` enviroment, and the `R` package `maotai` installed, and the python-R interface package `rpy2`.
+
+
+### Experiment notebooks
+We have included the experiments which does not require additional installations apart from the build-from-source scikit-learn, the three attached notebooks are used to recreate the experiments with the other non-parametric methods. Note that the blackbox optimization (while we supplied the code to run it), need to run separately, as it's multiprocess does not play well with Jupyter Notebook. 
+
+
+### Citing this work
+If you use this code for your work, please cite the following:
+
+```
+@inproceedings{dinari2022revisiting,
+  title={Revisiting DP-Means: Fast Scalable Algorithms via Parallelism and Delayed Cluster Creation},
+  author={Dinari, Or and Freifeld, Oren},
+  booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
+  year={2022}
+}
+```
+### License 
+Our code is licensed under the BDS-3-Clause license.
```

